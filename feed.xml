<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://kolchfa-aws.github.io/project/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kolchfa-aws.github.io/project/" rel="alternate" type="text/html" /><updated>2024-01-04T00:19:17+00:00</updated><id>https://kolchfa-aws.github.io/project/feed.xml</id><title type="html">OpenSearch</title><subtitle></subtitle><entry><title type="html">An update on the OpenSearch Project’s continued performance progress through 2.11</title><link href="https://kolchfa-aws.github.io/project/blog/opensearch-performance-improvements/" rel="alternate" type="text/html" title="An update on the OpenSearch Project’s continued performance progress through 2.11" /><published>2024-01-03T00:00:00+00:00</published><updated>2024-01-04T00:17:08+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/opensearch-performance-improvements</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/opensearch-performance-improvements/">&lt;style&gt;
.yellow-clr {
    background-color: #FFEFCC;
}

.orange-clr {
    background-color: #FCE2CF;
}

.light-orange-clr {
    background-color: #FDEFE5;
}

.green-clr {
    background-color: #c1f0c1;
}

.light-green-clr {
    background-color: #e3f8e3;
}
&lt;/style&gt;

&lt;p&gt;OpenSearch is a community-driven, open source search and analytics suite used by developers to ingest, search, visualize, and analyze data. &lt;a href=&quot;https://aws.amazon.com/blogs/opensource/stepping-up-for-a-truly-open-source-elasticsearch/&quot;&gt;Introduced in January 2021&lt;/a&gt;, the OpenSearch Project originated as an open source fork of Elasticsearch 7.10.2. OpenSearch 1.0 was released for production usage in &lt;a href=&quot;https://opensearch.org/blog/opensearch-general-availability-announcement/&quot;&gt;July 2021&lt;/a&gt; and is licensed under the &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;Apache License, Version 2.0&lt;/a&gt; (ALv2), with the complete codebase &lt;a href=&quot;https://github.com/opensearch-project&quot;&gt;published to GitHub&lt;/a&gt;.  The project has consistently focused on improving performance of its core open source engine for high volume indexing and low latency search operations. OpenSearch aims to provide the best experience for every user through driving down latency and improving efficiency.&lt;/p&gt;

&lt;p&gt;In this blog, we’ll share a comprehensive view of strategic enhancements and features in performance that OpenSearch has delivered to date. Additionally, we’ll provide a forward look at the &lt;a href=&quot;https://github.com/orgs/opensearch-project/projects/153/views/1&quot;&gt;planned roadmap&lt;/a&gt; of improvements in open source. We’ll compare the core engine performance of the latest OpenSearch version (OpenSearch 2.11)to the state just before the OpenSearch fork, Elasticsearch 7.10.2. We’ll highlight continuous advancements made in the OpenSearch core engine, ongoing feature enhancements centered around the popular log analytics and search use cases, and plans to drive improvements for which we are seeking community collaboration.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements-to-date&quot;&gt;Performance improvements to date&lt;/h2&gt;

&lt;p&gt;There are many dimensions to the performance improvements made in OpenSearch to date. We can categorize them into three high-level buckets:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Indexing performance&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Query performance&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The following image summarizes OpenSearch performance improvements.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2024-01-03-opensearch-performance-improvements/opensearch-performance.png&quot; alt=&quot;OpenSearch performance improvements since launch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Log analytics workloads are typically indexing heavy, often relying on specific resource-intensive queries. In contrast, search workloads have a more balanced distribution between indexing and query operations. Based on the analysis we’ll detail below, comparing Elasticsearch 7.10.2 to OpenSearch 2.11, we have seen a 25% improvement in indexing throughput, a 15–98% decrease in query latencies among some of the most popular query types, and now, with Zstandard compression, a 15–30% reduction in on-disk data size.&lt;/p&gt;

&lt;h3 id=&quot;indexing-performance-investments&quot;&gt;Indexing performance investments&lt;/h3&gt;

&lt;p&gt;Some of the key OpenSearch features this year delivered efficiency improvements in indexing performance. OpenSearch rearchitected the way indexing operations are performed to deliver &lt;strong&gt;&lt;em&gt;segment replication&lt;/em&gt;&lt;/strong&gt;—a physical replication method, which replicates index segments rather than source documents. Segment replication, a new replication strategy built on Lucene’s &lt;a href=&quot;https://blog.mikemccandless.com/2017/09/lucenes-near-real-time-segment-index.html&quot;&gt;Near-Real-Time (NRT) Segment Index Replication API&lt;/a&gt; was released as generally available in OpenSearch 2.7. Segment replication showed increased ingestion rate throughput of up to 25% when compared to default document replication. You can find a more detailed look at segment replication in the OpenSearch &lt;a href=&quot;https://opensearch.org/blog/segment-replication/&quot;&gt;segment replication blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In version 2.10, OpenSearch introduced &lt;strong&gt;&lt;em&gt;remote-backed storage&lt;/em&gt;&lt;/strong&gt;, allowing users to directly write these segments to object storage such as Amazon-managed S3 or Oracle Blob Storage to improve data durability. With remote-backed storage, in addition to storing data on a local disk, all the ingested data is stored in the configured remote store. At every refresh, the remote store also automatically becomes a point-in-time recovery point, helping users achieve a recovery point objective (RPO) of zero with the same durability properties of the configured remote store. To learn more, see the OpenSearch &lt;a href=&quot;https://opensearch.org/blog/remote-backed-storage/&quot;&gt;remote-backed storage blog&lt;/a&gt; .&lt;/p&gt;

&lt;h3 id=&quot;query-performance-investments&quot;&gt;Query performance investments&lt;/h3&gt;

&lt;p&gt;OpenSearch supports an extensive array of query types for different use cases, from comprehensive search capabilities to a broad spectrum of aggregations, filtering options, and sorting functionalities. One of the major query performance areas where OpenSearch has improved is helping vector queries perform at scale, given the rise in vector search popularity. OpenSearch’s vector engine offers fast, billion-scale vector searches with efficient latency and recall.&lt;/p&gt;

&lt;p&gt;Recent additions like scalar and product quantization reduced the cluster memory footprint up to 80%. The incorporation of native libraries (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nmslib&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;faiss&lt;/code&gt;) and HNSW with SIMD instructions has expedited vector indexing and search queries. At a large scale, tested with billions of documents, OpenSearch delivered a roughly 30% lower latency compared to Lucene ANN searches. For more information, see the &lt;a href=&quot;https://opensearch.org/blog/aws-knn-algorithms-workload-optimizations/&quot;&gt;vector search optimizations partner highlight&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’ve also continued to invest broadly in core query performance for popular query types used for log analytics, time-series data, and search. OpenSearch has demonstrated significant improvement since the fork from Elasticsearch 7.10.2 for many query types. The benchmarking we performed showed 15%–98% increase in performance across popular query operations such as match all, range queries, aggregation queries, and full-text queries. You can review detailed key benchmarking findings in the following sections.&lt;/p&gt;

&lt;h3 id=&quot;storage-investments&quot;&gt;Storage investments&lt;/h3&gt;

&lt;p&gt;Storage is another major factor that affects the overall efficiency of log analytics and search workloads. In OpenSearch 2.9 and later, customers can use Zstandard compression, resulting in a 30% reduction in on-disk data size while maintaining a near-identical CPU utilization pattern compared to the default compression. Some of the ongoing work, such as addition of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;match_only_text&lt;/code&gt; field (see &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/pull/11039&quot;&gt; #11039&lt;/a&gt;) has shown promising improvements of about 25% reduction for data on disk, primarily with text data field optimization, and should be available for users in the upcoming OpenSearch 2.12 release.&lt;/p&gt;

&lt;h2 id=&quot;measured-performance-improvements&quot;&gt;Measured performance improvements&lt;/h2&gt;

&lt;p&gt;To compare the performance between Elasticsearch 7.10.2 and OpenSearch 2.11, we ran query operations for widely used scenarios in log analytics, time series, and search. We ran the queries across clusters running each version and documented the resulting performance. The following are the key findings from this exercise.&lt;/p&gt;

&lt;h3 id=&quot;log-analytics&quot;&gt;Log analytics&lt;/h3&gt;

&lt;p&gt;For log analytics use cases, we used the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workload from the &lt;a href=&quot;https://opensearch.org/docs/latest/benchmark/index/&quot;&gt;OpenSearch Benchmark&lt;/a&gt;—a macro-benchmark utility within the OpenSearch Project—to replicate some of the common query operations. Here are the key highlights:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;match_all&lt;/code&gt; queries with sorting showed more than 20x performance boost across the board because of multiple improvements made in the area (see &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/pull/6321&quot;&gt;#6321&lt;/a&gt; and &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/pull/7244&quot;&gt;#7244&lt;/a&gt;) and other Lucene enhancements.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Queries for ascending and descending sort-after-timestamp saw a significant performance improvement of up to 70x overall. The optimizations introduced (such as &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/pull/6424&quot;&gt;#6424&lt;/a&gt; and &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/issues/8167&quot;&gt;#8167&lt;/a&gt;) extend across various numeric types, including but not limited to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;short&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;double&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;date&lt;/code&gt;, and others.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other popular queries such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;search_after&lt;/code&gt; saw about 60x reduction in latency, attributed to the improvements made in the area involving optimally skipping segments during search (see &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/pull/7453&quot;&gt;#7453&lt;/a&gt;). The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;search_after&lt;/code&gt; queries can be used as the recommended alternative to scroll queries for a better search experience.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;time-series&quot;&gt;Time series&lt;/h3&gt;

&lt;p&gt;In the context of aggregations over range and time-series data, we used the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workloads from OpenSearch Benchmark to benchmark various popular use cases. Here are the key highlights:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Range queries, popular for aggregation use cases, exhibited about 50% to 75% improvement, attributed to system upgrades such as Lucene (from v8.8 in Elasticsearch 7.10.2 to v9.7 in OpenSearch 2.11) and JDK (from JDK15 in Elasticsearch 7.10.2 to JDK17 in OpenSearch 2.11).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hourly aggregations and multi-term aggregations also demonstrated improvement, varying from 5% to 35%, attributed to similar time-series improvements discussed previously.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;date_histograms&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;date_histogram_agg&lt;/code&gt; queries exhibited either comparable or slightly decreased performance, ranging from 5% to around 20% in multi-node environments. These issues are actively being addressed as part of the ongoing project efforts (see &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/pull/11083&quot;&gt;#11083&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;search&quot;&gt;Search&lt;/h3&gt;

&lt;p&gt;In the realm of text queries, we used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pmc&lt;/code&gt; workloads from OpenSearch Benchmark to emulate various common use cases. Here are the noteworthy highlights:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Phrase and term queries for text search showed improved latency, with 25% to 65% reduction, underscoring their improved effectiveness.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Popular queries related to scrolling exhibited about 15% lower latency, further improving the overall user experience.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;additional-optional-performance-enhancing-features-available-in-version-211&quot;&gt;Additional optional performance-enhancing features available in version 2.11&lt;/h2&gt;

&lt;p&gt;The core engine optimizations discussed previously is available by default. Additionally, OpenSearch 2.11 includes a few key performance-enhancing features, which can be optionally enabled by users. These features were not available in prior versions, so we separately benchmarked performance with those features individually enabled, resulting in the following findings:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/issues/9241&quot;&gt;&lt;strong&gt;LogByteSize merge policy&lt;/strong&gt;&lt;/a&gt;: Showed a 40-70% improvement in ascending and descending sort queries, which is advantageous for time-series data with timestamp sorting and minimal timestamp overlap between segments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://opensearch.org/docs/latest/im-plugin/index-codecs/#changing-an-index-codec&quot;&gt;&lt;strong&gt;Zstandard compression&lt;/strong&gt;&lt;/a&gt;: This addition empowers OpenSearch users with the new Zstandard compression codecs for their data, resulting in 30% reduction in on-disk data size while maintaining a near-identical CPU utilization pattern compared to the default compression.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://opensearch.org/blog/concurrent_segment_search/&quot;&gt;&lt;strong&gt;Concurrent segment search&lt;/strong&gt;&lt;/a&gt;: Enabling every shard-level request to concurrently search across segments during the query phase resulted in latency reduction across multiple query types. Aggregate queries showed a 50% to 70% improvement, range queries showed a 65% improvement, and aggregation queries with hourly data aggregations showed a 50% improvement.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-roadmap&quot;&gt;Future roadmap&lt;/h2&gt;

&lt;p&gt;The OpenSearch project remains steadfast in its commitment to continuously improve the core engine performance in search, ingestion, and storage operations. &lt;a href=&quot;https://github.com/orgs/opensearch-project/projects/153/views/1&quot;&gt;The OpenSearch Project roadmap on GitHub&lt;/a&gt; is constantly evolving, and we are excited to share it with you. This roadmap places a special emphasis on the core engine advancements while also encompassing critical areas like benchmarking and query visibility. As part of our ongoing commitment, we plan to consistently update this roadmap with both short- and long-term improvement plans. Keeping performance excellence at the forefront of our investments, OpenSearch users can anticipate a series of impactful improvements on the horizon through new releases in 2024, starting with OpenSearch 2.12.&lt;/p&gt;

&lt;p&gt;In the upcoming releases, we will continue to improve the core engine by targeting specific query types. We will also undertake broad strategic initiatives to further enhance the core engine through Protobuf integration, query rewrites, tiered caching, and SIMD and RUST implementations. In addition to the core engine, OpenSearch is also committed to improving its tooling capabilities. One such improvement that we’re currently working on is the query insights functionality, which helps identify the top N queries that impact performance. Additionally, OpenSearch is working on making its benchmarks easier for community members to use. For a comprehensive list of investments, additional improvements, or to provide feedback, please check out the OpenSearch performance &lt;a href=&quot;https://github.com/orgs/opensearch-project/projects/153/views/1&quot;&gt;roadmap&lt;/a&gt; on GitHub.&lt;/p&gt;

&lt;p&gt;This concludes the main summary of OpenSearch performance improvements to-date. The following appendix sections cover details on running benchmarks for readers interested in replicating any run.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;appendix-detailed-execution-and-results&quot;&gt;Appendix: Detailed execution and results&lt;/h2&gt;

&lt;p&gt;If you are interested in the details on the performance benchmarks used, exploring the methodologies behind their execution, and examining the comprehensive results, keep reading. Also, for OpenSearch users interested in establishing benchmarks and replicating these runs, we’ve provided comprehensive setup details alongside each result for your convenience. This section provides the core engine performance comparison between the latest OpenSearch version (OpenSearch 2.11) and the state just before the OpenSearch fork, Elasticsearch 7.10.2, with a mid-point performance measurement on OpenSearch 2.3.&lt;/p&gt;

&lt;h3 id=&quot;opensearch-benchmark-and-workloads&quot;&gt;OpenSearch Benchmark and workloads&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/opensearch-project/opensearch-benchmark&quot;&gt;&lt;strong&gt;OpenSearch Benchmark&lt;/strong&gt;&lt;/a&gt; serves as a macro-benchmark utility within the OpenSearch Project. Users and developers of OpenSearch can use this tool to generate and visualize performance metrics from an OpenSearch cluster for various purposes, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Monitoring the overall performance of an OpenSearch cluster.&lt;/li&gt;
  &lt;li&gt;Assisting in decisions regarding the benefits of upgrading the cluster to a new version.&lt;/li&gt;
  &lt;li&gt;Assessing the potential impact on the cluster resulting from changes to the workflows, such as modifications of the index mappings or change in queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/opensearch-project/opensearch-benchmark-workloads&quot;&gt;&lt;strong&gt;OpenSearch Benchmark workloads&lt;/strong&gt;&lt;/a&gt; are comprised of one or multiple benchmarking scenarios. A workload typically includes the ingestion of one or more data corpora into indexes and a collection of queries and operations that are executed as a part of the benchmark.&lt;/p&gt;

&lt;p&gt;We used the following workloads for performance evaluation, encompassing aspects such as text/term queries, sorting, aggregations, histogram, and ranges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/opensearch-project/opensearch-benchmark-workloads/tree/main/http_logs#http-logs-workload&quot;&gt;HTTP logs workload&lt;/a&gt;: This workload is based on &lt;a href=&quot;http://ita.ee.lbl.gov/html/contrib/WorldCup.html&quot;&gt;Web server logs from the 1998 Football world cup&lt;/a&gt;. It is used for evaluating the performance of (Web) server logs, most in line with the OpenSearch log analytics use case.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/opensearch-project/opensearch-benchmark-workloads/tree/main/nyc_taxis#nyc-taxis-workload&quot;&gt;NYC taxis workload&lt;/a&gt;: This workload contains the rides taken in &lt;a href=&quot;https://www.nyc.govs.ite/tlc/about/tlc-trip-record-data.page&quot;&gt;yellow taxis in New York in 2015&lt;/a&gt;. It is used for evaluating the performance for highly structured data. It is useful for aggregation and date histogram use cases for time-series data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/opensearch-project/opensearch-benchmark-workloads/tree/main/pmc#pmc-workload&quot;&gt;PMC workload&lt;/a&gt;: This workload contains data retrieved from &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/tools/ftp/&quot;&gt;PubMed Central (PMC)&lt;/a&gt;. It is used for evaluating the performance of full-text search, in line with the OpenSearch search use case.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The configurations specific to the setup for each evaluation are provided along with the results in the following sections.&lt;/p&gt;

&lt;h2 id=&quot;comparative-baseline-analysis-elasticsearch-7102-vs-opensearch-23-vs-opensearch-211&quot;&gt;Comparative baseline analysis: Elasticsearch 7.10.2 vs. OpenSearch 2.3 vs. OpenSearch 2.11&lt;/h2&gt;

&lt;p&gt;We compared the performance of Elasticsearch 7.10.2 (pre-fork), OpenSearch 2.3 (an interim release), and OpenSearch 2.11 (latest version at the time of testing). This analysis covers three workloads (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http-logs&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc-taxis&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pmc&lt;/code&gt;) used to assess performance across different use cases. The goal is to provide comparable core engine performance metrics since the Elasticsearch 7.10.2 fork. The benchmarks in the following sections show averages from seven days of data, generated during nightly runs using the &lt;a href=&quot;https://github.com/opensearch-project/opensearch-benchmark&quot;&gt;OpenSearch Benchmark&lt;/a&gt;, intentionally excluding outliers. In the detailed results section, each table contains the percentage improvement column. This column emphasizes the advancements of OpenSearch 2.11 over the previous releases, with positive values indicating improvement and negative values indicating regression.&lt;/p&gt;

&lt;h3 id=&quot;detailed-results-elasticsearch-7102-vs-opensearch-23-vs-opensearch-211deep-dive-into-single-shard-performance&quot;&gt;Detailed results: Elasticsearch 7.10.2 vs. OpenSearch 2.3 vs. OpenSearch 2.11—deep dive into single shard performance&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: To eliminate variables introduced at the coordination level and concentrate on data node query performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt; : 1 data node (r5.xlarge) with 32 GB RAM and 16 GB heap. Index settings: 1 Shard and 0 replicas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workload results:&lt;/strong&gt; The following table illustrates a benchmark comparison for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workload between Elasticsearch 7.10.2, OpenSearch 2.3, and OpenSearch 2.11. It includes the 90th percentile of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;took&lt;/code&gt; time latency measurements for each (p90) and the observed percentage improvements.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;ES 7.10.2 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.3.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 improvement (vs. OS 2.3.0)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 improvement (vs. ES 7.10.2)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;200s-in-range&lt;/td&gt;
        &lt;td&gt;13&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;5.33&lt;/td&gt;
        &lt;td&gt;-7%&lt;/td&gt;
        &lt;td&gt;59%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;400s-in-range&lt;/td&gt;
        &lt;td&gt;2.73&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;2.53&lt;/td&gt;
        &lt;td&gt;-26%&lt;/td&gt;
        &lt;td&gt;7%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;asc_sort_size&lt;/td&gt;
        &lt;td&gt;4,262.2&lt;/td&gt;
        &lt;td&gt;4,471&lt;/td&gt;
        &lt;td&gt;4.6&lt;/td&gt;
        &lt;td&gt;100%&lt;/td&gt;
        &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;asc_sort_timestamp&lt;/td&gt;
        &lt;td&gt;10.73&lt;/td&gt;
        &lt;td&gt;785&lt;/td&gt;
        &lt;td&gt;6.47&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
        &lt;td&gt;40%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;asc_sort_with_after_timestamp&lt;/td&gt;
        &lt;td&gt;4,576&lt;/td&gt;
        &lt;td&gt;5,368&lt;/td&gt;
        &lt;td&gt;34.47&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;2.91&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
        &lt;td&gt;-3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;desc_sort_size&lt;/td&gt;
        &lt;td&gt;3,800.4&lt;/td&gt;
        &lt;td&gt;3,994&lt;/td&gt;
        &lt;td&gt;9.53&lt;/td&gt;
        &lt;td&gt;100%&lt;/td&gt;
        &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-orange-clr&quot;&gt;
        &lt;td&gt;desc_sort_timestamp&lt;/td&gt;
        &lt;td&gt;39.18&lt;/td&gt;
        &lt;td&gt;5,228&lt;/td&gt;
        &lt;td&gt;58.8&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
        &lt;td&gt;-50%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;desc_sort_with_after_timestamp&lt;/td&gt;
        &lt;td&gt;5,824.27&lt;/td&gt;
        &lt;td&gt;6,925&lt;/td&gt;
        &lt;td&gt;87.8&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
        &lt;td&gt;98%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;hourly_agg&lt;/td&gt;
        &lt;td&gt;9,387.55&lt;/td&gt;
        &lt;td&gt;9,640&lt;/td&gt;
        &lt;td&gt;9,112.4&lt;/td&gt;
        &lt;td&gt;5%&lt;/td&gt;
        &lt;td&gt;3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;multi_term_agg&lt;/td&gt;
        &lt;td&gt;N/A&lt;/td&gt;
        &lt;td&gt;14,703&lt;/td&gt;
        &lt;td&gt;9,669.8&lt;/td&gt;
        &lt;td&gt;34%&lt;/td&gt;
        &lt;td&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;range&lt;/td&gt;
        &lt;td&gt;28.18&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
        &lt;td&gt;13.4&lt;/td&gt;
        &lt;td&gt;-12%&lt;/td&gt;
        &lt;td&gt;52%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;scroll&lt;/td&gt;
        &lt;td&gt;213.91&lt;/td&gt;
        &lt;td&gt;173&lt;/td&gt;
        &lt;td&gt;197.4&lt;/td&gt;
        &lt;td&gt;-14%&lt;/td&gt;
        &lt;td&gt;8%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;term&lt;/td&gt;
        &lt;td&gt;3.45&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;3.4&lt;/td&gt;
        &lt;td&gt;-13%&lt;/td&gt;
        &lt;td&gt;1%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; workload results:&lt;/strong&gt; The following table illustrates a benchmark comparison for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; workload between Elasticsearch 7.10.2, OpenSearch 2.3, and OpenSearch 2.11. It includes the 90th percentile of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;took&lt;/code&gt; time latency measurements for each (p90) and the observed percentage improvements.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;ES 7.10.2 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.3.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 improvement (vs. OS 2.3.0)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 improvement (vs. ES 7.10.2)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;autohisto_agg&lt;/td&gt;
        &lt;td&gt;559.13&lt;/td&gt;
        &lt;td&gt;596&lt;/td&gt;
        &lt;td&gt;554.6&lt;/td&gt;
        &lt;td&gt;7%&lt;/td&gt;
        &lt;td&gt;1%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;date_histogram_agg&lt;/td&gt;
        &lt;td&gt;562.4&lt;/td&gt;
        &lt;td&gt;584&lt;/td&gt;
        &lt;td&gt;545.07&lt;/td&gt;
        &lt;td&gt;7%&lt;/td&gt;
        &lt;td&gt;3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;4.73&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;5.07&lt;/td&gt;
        &lt;td&gt;15%&lt;/td&gt;
        &lt;td&gt;-7%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-orange-clr&quot;&gt;
        &lt;td&gt;distance_amount_agg&lt;/td&gt;
        &lt;td&gt;13181&lt;/td&gt;
        &lt;td&gt;12796&lt;/td&gt;
        &lt;td&gt;15285&lt;/td&gt;
        &lt;td&gt;-19%&lt;/td&gt;
        &lt;td&gt;-16%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;range&lt;/td&gt;
        &lt;td&gt;654.67&lt;/td&gt;
        &lt;td&gt;213&lt;/td&gt;
        &lt;td&gt;213.4&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
        &lt;td&gt;67%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pmc&lt;/code&gt; workload results:&lt;/strong&gt; The following table illustrates a benchmark comparison for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pmc&lt;/code&gt; workload between Elasticsearch 7.10.2, OpenSearch 2.3, and OpenSearch 2.11. It includes the 90th percentile of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;took&lt;/code&gt; time latency measurements for each (p90) and the observed percentage improvements.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;ES 7.10.2 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.3.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 improvement (vs. OS 2.3.0)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 improvement (vs. ES 7.10.2)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;articles_monthly_agg_cached&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;2.4&lt;/td&gt;
        &lt;td&gt;-20%&lt;/td&gt;
        &lt;td&gt;-20%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;articles_monthly_agg_uncached&lt;/td&gt;
        &lt;td&gt;26.5&lt;/td&gt;
        &lt;td&gt;27&lt;/td&gt;
        &lt;td&gt;27.8&lt;/td&gt;
        &lt;td&gt;-3%&lt;/td&gt;
        &lt;td&gt;-5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;6.5&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;5.2&lt;/td&gt;
        &lt;td&gt;13%&lt;/td&gt;
        &lt;td&gt;20%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;phrase&lt;/td&gt;
        &lt;td&gt;8.25&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;6.4&lt;/td&gt;
        &lt;td&gt;-7%&lt;/td&gt;
        &lt;td&gt;22%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;scroll&lt;/td&gt;
        &lt;td&gt;894.5&lt;/td&gt;
        &lt;td&gt;857&lt;/td&gt;
        &lt;td&gt;753.8&lt;/td&gt;
        &lt;td&gt;12%&lt;/td&gt;
        &lt;td&gt;16%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;term&lt;/td&gt;
        &lt;td&gt;9&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;5.6&lt;/td&gt;
        &lt;td&gt;-12%&lt;/td&gt;
        &lt;td&gt;38%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;detailed-results-elasticsearch-7102-vs-opensearch-23-vs-opensearch-211deep-dive-into-multiple-shard-performance&quot;&gt;Detailed results: Elasticsearch 7.10.2 vs. OpenSearch 2.3 vs. OpenSearch 2.11—deep dive into multiple shard performance&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: To introduce the coordination layer with parallel search operations extending across multiple nodes with primary shards.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; 3 data nodes(r5.xlarge) with 32 GB RAM and 16 GB heap. 3 cluster manager nodes (c5.xlarge) with 8 GB RAM and 4 GB heap. Index settings: 3 Shards and 0 replicass.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workload results:&lt;/strong&gt; The following table illustrates a benchmark comparison for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workload between Elasticsearch 7.10.2, OpenSearch 2.3, and OpenSearch 2.11. It includes the 90th percentile of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;took&lt;/code&gt; time latency measurements for each (p90) and the observed percentage improvements.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;ES 7.10.2 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.3.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 improvement (vs. OS 2.3.0)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 Improvement (vs. ES 7.10.2)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;200s-in-range&lt;/td&gt;
        &lt;td&gt;9.8&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;6.85&lt;/td&gt;
        &lt;td&gt;-14%&lt;/td&gt;
        &lt;td&gt;30%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;400s-in-range&lt;/td&gt;
        &lt;td&gt;5.8&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;5.5&lt;/td&gt;
        &lt;td&gt;-10%&lt;/td&gt;
        &lt;td&gt;5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;asc_sort_size&lt;/td&gt;
        &lt;td&gt;1,451.13&lt;/td&gt;
        &lt;td&gt;1,602&lt;/td&gt;
        &lt;td&gt;8.35&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;asc_sort_timestamp&lt;/td&gt;
        &lt;td&gt;10.4&lt;/td&gt;
        &lt;td&gt;291.5&lt;/td&gt;
        &lt;td&gt;10.58&lt;/td&gt;
        &lt;td&gt;96%&lt;/td&gt;
        &lt;td&gt;-2%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;asc_sort_with_after_timestamp&lt;/td&gt;
        &lt;td&gt;1,488.25&lt;/td&gt;
        &lt;td&gt;1,910.5&lt;/td&gt;
        &lt;td&gt;25.38&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
        &lt;td&gt;98%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;6.3&lt;/td&gt;
        &lt;td&gt;-5%&lt;/td&gt;
        &lt;td&gt;-5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;desc_sort_size&lt;/td&gt;
        &lt;td&gt;1,281.3&lt;/td&gt;
        &lt;td&gt;1,431&lt;/td&gt;
        &lt;td&gt;13.91&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
        &lt;td&gt;99%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;desc_sort_timestamp&lt;/td&gt;
        &lt;td&gt;34.4&lt;/td&gt;
        &lt;td&gt;1,878.5&lt;/td&gt;
        &lt;td&gt;91.9&lt;/td&gt;
        &lt;td&gt;95%&lt;/td&gt;
        &lt;td&gt;-167%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;desc_sort_with_after_timestamp&lt;/td&gt;
        &lt;td&gt;1,887.7&lt;/td&gt;
        &lt;td&gt;2,480&lt;/td&gt;
        &lt;td&gt;85.78&lt;/td&gt;
        &lt;td&gt;97%&lt;/td&gt;
        &lt;td&gt;95%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;hourly_agg&lt;/td&gt;
        &lt;td&gt;2,566.9&lt;/td&gt;
        &lt;td&gt;3,115&lt;/td&gt;
        &lt;td&gt;2,937&lt;/td&gt;
        &lt;td&gt;6%&lt;/td&gt;
        &lt;td&gt;-14%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;multi_term_agg&lt;/td&gt;
        &lt;td&gt;N/A&lt;/td&gt;
        &lt;td&gt;5205&lt;/td&gt;
        &lt;td&gt;3603&lt;/td&gt;
        &lt;td&gt;31%&lt;/td&gt;
        &lt;td&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;range&lt;/td&gt;
        &lt;td&gt;18.1&lt;/td&gt;
        &lt;td&gt;9&lt;/td&gt;
        &lt;td&gt;8.95&lt;/td&gt;
        &lt;td&gt;1%&lt;/td&gt;
        &lt;td&gt;51%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;scroll&lt;/td&gt;
        &lt;td&gt;340.1&lt;/td&gt;
        &lt;td&gt;267&lt;/td&gt;
        &lt;td&gt;323.85&lt;/td&gt;
        &lt;td&gt;-21%&lt;/td&gt;
        &lt;td&gt;5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;term&lt;/td&gt;
        &lt;td&gt;5.8&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;6.45&lt;/td&gt;
        &lt;td&gt;-8%&lt;/td&gt;
        &lt;td&gt;-11%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; workload results:&lt;/strong&gt; The following table illustrates a benchmark comparison for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; workload between Elasticsearch 7.10.2, OpenSearch 2.3, and OpenSearch 2.11. It includes the 90th percentile of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;took&lt;/code&gt; time latency measurements for each (p90) and the observed percentage improvements.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;ES 7.10.2 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.3.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 Improvement (vs. OS 2.3.0)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 Improvement (vs. ES 7.10.2)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-orange-clr&quot;&gt;
        &lt;td&gt;autohisto_agg&lt;/td&gt;
        &lt;td&gt;208.92&lt;/td&gt;
        &lt;td&gt;217&lt;/td&gt;
        &lt;td&gt;212.93&lt;/td&gt;
        &lt;td&gt;2%&lt;/td&gt;
        &lt;td&gt;-2%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-orange-clr&quot;&gt;
        &lt;td&gt;date_histogram_agg&lt;/td&gt;
        &lt;td&gt;198.77&lt;/td&gt;
        &lt;td&gt;218&lt;/td&gt;
        &lt;td&gt;209.4&lt;/td&gt;
        &lt;td&gt;4%&lt;/td&gt;
        &lt;td&gt;-5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;8.46&lt;/td&gt;
        &lt;td&gt;7&lt;/td&gt;
        &lt;td&gt;9.67&lt;/td&gt;
        &lt;td&gt;-38%&lt;/td&gt;
        &lt;td&gt;-14%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-orange-clr&quot;&gt;
        &lt;td&gt;distance_amount_agg&lt;/td&gt;
        &lt;td&gt;4,131&lt;/td&gt;
        &lt;td&gt;4,696&lt;/td&gt;
        &lt;td&gt;5,067.4&lt;/td&gt;
        &lt;td&gt;-8%&lt;/td&gt;
        &lt;td&gt;-23%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;range&lt;/td&gt;
        &lt;td&gt;281.62&lt;/td&gt;
        &lt;td&gt;73&lt;/td&gt;
        &lt;td&gt;79.53&lt;/td&gt;
        &lt;td&gt;-9%&lt;/td&gt;
        &lt;td&gt;72%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;PMC workload results:&lt;/strong&gt; The following table illustrates a benchmark comparison for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pmc&lt;/code&gt; workload between Elasticsearch 7.10.2, OpenSearch 2.3, and OpenSearch 2.11. It includes the 90th percentile of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;took&lt;/code&gt; time latency measurements for each (p90) and the observed percentage improvements.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;ES 7.10.2 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.3.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 improvement (vs. OS 2.3.0)&lt;/th&gt;
        &lt;th&gt;OS 2.11.0 improvement (vs. ES 7.10.2)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;articles_monthly_agg_cached&lt;/td&gt;
        &lt;td&gt;3.55&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;3.71&lt;/td&gt;
        &lt;td&gt;-24%&lt;/td&gt;
        &lt;td&gt;-5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;articles_monthly_agg_uncached&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
        &lt;td&gt;12.43&lt;/td&gt;
        &lt;td&gt;-4%&lt;/td&gt;
        &lt;td&gt;-4%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;9&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;6.79&lt;/td&gt;
        &lt;td&gt;-13%&lt;/td&gt;
        &lt;td&gt;25%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;phrase&lt;/td&gt;
        &lt;td&gt;8.18&lt;/td&gt;
        &lt;td&gt;7&lt;/td&gt;
        &lt;td&gt;7.14&lt;/td&gt;
        &lt;td&gt;-2%&lt;/td&gt;
        &lt;td&gt;13%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;scroll&lt;/td&gt;
        &lt;td&gt;755.18&lt;/td&gt;
        &lt;td&gt;593&lt;/td&gt;
        &lt;td&gt;642.79&lt;/td&gt;
        &lt;td&gt;-8%&lt;/td&gt;
        &lt;td&gt;15%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;term&lt;/td&gt;
        &lt;td&gt;9&lt;/td&gt;
        &lt;td&gt;7&lt;/td&gt;
        &lt;td&gt;7.14&lt;/td&gt;
        &lt;td&gt;-2%&lt;/td&gt;
        &lt;td&gt;21%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;elevating-performance-with-new-features-available-in-opensearch-211&quot;&gt;Elevating performance with new features available in OpenSearch 2.11&lt;/h2&gt;

&lt;p&gt;The following sections present OpenSearch 2.11 features that improve performance.&lt;/p&gt;

&lt;h3 id=&quot;logbytesize-merge-policy&quot;&gt;LogByteSize merge policy&lt;/h3&gt;

&lt;p&gt;In the realm of log analytics, the Tiered merge policy has been a cornerstone for efficient shard merges. In OpenSearch 2.11 we introduced the LogByteSize merge policy. This new approach consistently merges adjacent segments, proving especially advantageous for time-series data characterized by timestamp sorting and minimal timestamp overlap between segments.&lt;/p&gt;

&lt;p&gt;The following are the key findings from this exercise.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Timestamp queries with ascending sort had over 75% improvement. This transformation is attributable to the impactful contribution of enhancement &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/issues/9241&quot;&gt;#9241&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;About 40% enhancement in descending sort timestamp queries, surpassing tiered merge policy.&lt;/li&gt;
  &lt;li&gt;Use cases around ascending and descending sort with_after_timestamp saw regression which is a known case for smaller workload with this merge policy.&lt;/li&gt;
  &lt;li&gt;Other common use cases for log analytics, such as Multi-term aggregation, hourly_agg, range, and scroll queries exhibited comparable performance, with a subtle improvement of less than 5% attributed to the new segment merge policy.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;detailed-results-os-211-tiered-merge-policy-vs-os-211-logbytesize-merge-policy&quot;&gt;Detailed Results: OS 2.11 Tiered merge policy vs OS 2.11 LogByteSize merge policy&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: 3 data nodes (r5.xlarge) with 32 GB RAM and 16 GB heap. 3 cluster manager nodes (c5.xlarge) with 8 GB RAM and 4 GB heap. Index settings: 3 Shards and 0 replicas, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_segment_size&lt;/code&gt;: 500 MB, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;refresh_interval&lt;/code&gt;: 1 s.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workload results:&lt;/strong&gt; The following table illustrates a benchmark comparison for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workload for OpenSearch 2.11 with Tiered merge policy vs. LogByteSize merge policy. It includes the 90th percentile of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;took&lt;/code&gt; time latency measurements for each (p90) and the observed percentage improvements.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;OS 2.11---Tiered (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11---LogByteSize (p90_value)&lt;/th&gt;
        &lt;th&gt;% improvement (vs. Tiered merge policy)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;200s-in-range&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;400s-in-range&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;-20%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;asc_sort_size&lt;/td&gt;
        &lt;td&gt;9&lt;/td&gt;
        &lt;td&gt;8&lt;/td&gt;
        &lt;td&gt;11%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;asc_sort_timestamp&lt;/td&gt;
        &lt;td&gt;34&lt;/td&gt;
        &lt;td&gt;8&lt;/td&gt;
        &lt;td&gt;76%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;orange-clr&quot;&gt;
        &lt;td&gt;asc_sort_with_after_timestamp&lt;/td&gt;
        &lt;td&gt;13&lt;/td&gt;
        &lt;td&gt;68&lt;/td&gt;
        &lt;td&gt;-423%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;7&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;14%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;desc_sort_size&lt;/td&gt;
        &lt;td&gt;11&lt;/td&gt;
        &lt;td&gt;10&lt;/td&gt;
        &lt;td&gt;9%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;desc_sort_timestamp&lt;/td&gt;
        &lt;td&gt;29&lt;/td&gt;
        &lt;td&gt;17&lt;/td&gt;
        &lt;td&gt;41%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;orange-clr&quot;&gt;
        &lt;td&gt;desc_sort_with_after_timestamp&lt;/td&gt;
        &lt;td&gt;35&lt;/td&gt;
        &lt;td&gt;130&lt;/td&gt;
        &lt;td&gt;-271%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;hourly_agg&lt;/td&gt;
        &lt;td&gt;2816&lt;/td&gt;
        &lt;td&gt;2809&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;multi_term_agg&lt;/td&gt;
        &lt;td&gt;2739&lt;/td&gt;
        &lt;td&gt;2800&lt;/td&gt;
        &lt;td&gt;-2%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;range&lt;/td&gt;
        &lt;td&gt;7&lt;/td&gt;
        &lt;td&gt;8&lt;/td&gt;
        &lt;td&gt;-14%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;scroll&lt;/td&gt;
        &lt;td&gt;349&lt;/td&gt;
        &lt;td&gt;344&lt;/td&gt;
        &lt;td&gt;1%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;term&lt;/td&gt;
        &lt;td&gt;7&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;14%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;zsandard-codec-compression&quot;&gt;Zsandard codec compression&lt;/h3&gt;

&lt;p&gt;This addition empowers OpenSearch users with the new Zstandard compression codecs for their data. Users can specify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zstd&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zstd_no_dict&lt;/code&gt; in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.codec&lt;/code&gt; setting during index creation or &lt;a href=&quot;https://opensearch.org/docs/latest/im-plugin/index-codecs/#changing-an-index-codec&quot;&gt;modify the codecs for existing indexes&lt;/a&gt;. OpenSearch will continue to support the existing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zlib&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lz4&lt;/code&gt; codecs, with the default as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lz4&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The following sections contain benchmarking results, representing the average over 5 days of data from nightly runs using the OpenSearch Benchmark.&lt;/p&gt;

&lt;h4 id=&quot;highlights&quot;&gt;Highlights&lt;/h4&gt;

&lt;p&gt;Here are the key highlights:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A notable increase in the ingestion throughput, ranging from 5% to 8%, attributed to the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zstd&lt;/code&gt; codec. This enhancement owes its success to codec-related pull requests (&lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/pull/7908&quot;&gt;#7908&lt;/a&gt;, &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/issues/7805&quot;&gt;#7805&lt;/a&gt;, and &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/issues/7555&quot;&gt;#7555&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;About 30% reduction in on-disk data size, surpassing the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lz4&lt;/code&gt; codec for unparalleled efficiency, all while maintaining a near-identical CPU utilization pattern compared to the default compression.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The search p90 latencies remained virtually unchanged, with negligible differences of less than 2% in a few areas.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;detailed-results-opensearch-211-default-lz4-compression-vs-opensearch-211-zstd-compression-using-multiple-shards&quot;&gt;Detailed Results: OpenSearch 2.11 default (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lz4&lt;/code&gt;) compression vs. OpenSearch 2.11 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zstd&lt;/code&gt; compression using multiple shards&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; 3 data nodes (r5.xlarge) with 32 GB RAM and 16 GB heap. 3 cluster manager nodes (c5.xlarge) with 8 GB RAM and 4G Heap. Index settings: 3 Shards and 0 replicas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Indexing throughput results (docs/sec)&lt;/strong&gt;: The following table presents the indexing throughput comparison of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; workloads for OpenSearch 2.11 with default vs. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zstd&lt;/code&gt; codec enabled, including its percentage improvement.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Workload&lt;/th&gt;
        &lt;th&gt;OS 2.11-default-codec (mean_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11-zstd-codec (mean_value)&lt;/th&gt;
        &lt;th&gt;% improvement (vs. default-codec)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;http_logs&lt;/td&gt;
        &lt;td&gt;209,959.75&lt;/td&gt;
        &lt;td&gt;220,948&lt;/td&gt;
        &lt;td&gt;5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;light-green-clr&quot;&gt;
        &lt;td&gt;nyc_taxis&lt;/td&gt;
        &lt;td&gt;118,123.5&lt;/td&gt;
        &lt;td&gt;127,131&lt;/td&gt;
        &lt;td&gt;8%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; search workload results&lt;/strong&gt;: The following table illustrates a benchmark comparison for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; workload for OpenSearch 2.11 with default codec vs. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zstd&lt;/code&gt; codec enabled, including percentage improvement.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;OS 2.11-default-codec (p90_value)&lt;/th&gt;
        &lt;th&gt;OS 2.11-zstd-codec (p90_value)&lt;/th&gt;
        &lt;th&gt;% improvement (vs. default-codec)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;autohisto_agg&lt;/td&gt;
        &lt;td&gt;216.75&lt;/td&gt;
        &lt;td&gt;208&lt;/td&gt;
        &lt;td&gt;4%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;date_histogram_agg&lt;/td&gt;
        &lt;td&gt;211.25&lt;/td&gt;
        &lt;td&gt;205.5&lt;/td&gt;
        &lt;td&gt;3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;8&lt;/td&gt;
        &lt;td&gt;7.5&lt;/td&gt;
        &lt;td&gt;6%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;distance_amount_agg&lt;/td&gt;
        &lt;td&gt;5,012&lt;/td&gt;
        &lt;td&gt;4,980&lt;/td&gt;
        &lt;td&gt;1%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;range&lt;/td&gt;
        &lt;td&gt;74.5&lt;/td&gt;
        &lt;td&gt;77.5&lt;/td&gt;
        &lt;td&gt;-4%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;detailed-results-index-data-size-on-disk-bytes-with-zstandard-compression-using-a-single-shard&quot;&gt;Detailed results: Index data size on disk (bytes) with Zstandard compression using a single shard&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; OpenSearch 2.11.0, single node (r5.xlarge) with 32 GB RAM and 16 GB heap. Index settings: 1 Shard and 0 replicas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data size of disk results (bytes)&lt;/strong&gt;: The following table illustrates a benchmark comparison of the on-disk data size for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pmc&lt;/code&gt; workloads for OpenSearch 2.11 with default vs. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zstd&lt;/code&gt; codec enabled, including percentage improvement.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;&lt;/th&gt;
        &lt;th&gt;Default compression (bytes)&lt;/th&gt;
        &lt;th&gt;ZSTD (bytes)&lt;/th&gt;
        &lt;th&gt;ZSTD_NO_DICT (bytes)&lt;/th&gt;
        &lt;th&gt;ZSTD improvement (vs. default-codec)&lt;/th&gt;
        &lt;th&gt;ZSTD_NO_DICT improvement (vs. default-codec)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;http_logs&lt;/td&gt;
        &lt;td&gt;20,056,770,878.5&lt;/td&gt;
        &lt;td&gt;15,800,734,037&lt;/td&gt;
        &lt;td&gt;16,203,187,551&lt;/td&gt;
        &lt;td&gt;21%&lt;/td&gt;
        &lt;td&gt;19%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;pmc&lt;/td&gt;
        &lt;td&gt;20,701,211,614.5&lt;/td&gt;
        &lt;td&gt;15,608,881,718.5&lt;/td&gt;
        &lt;td&gt;15,822,040,185&lt;/td&gt;
        &lt;td&gt;25%&lt;/td&gt;
        &lt;td&gt;24%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;concurrent-search-improvement-experimental-in-211&quot;&gt;Concurrent search improvement (&lt;em&gt;Experimental in 2.11&lt;/em&gt;)&lt;/h3&gt;

&lt;p&gt;OpenSearch users can now achieve better execution speed with concurrent segment search launched as experimental in OpenSearch 2.11. By default, OpenSearch processes a request sequentially across all the data segments on each shard during the query phase of a search request execution. With concurrent search, every shard-level request can concurrently search across segments during the query phase. Each shard divides its segments into &lt;a href=&quot;https://opensearch.org/blog/concurrent_segment_search/&quot;&gt;multiple slices&lt;/a&gt;, where each slice serves as a unit of work executed in parallel on a separate thread. Therefore, the slice count governs the maximum degree of parallelism for a shard-level request. After all the slices finish their tasks, OpenSearch executes a reduce operation on the slices, merging them to generate the final result for the shard-level request.&lt;/p&gt;

&lt;p&gt;The benchmark results presented below show the benefits of concurrent search in action. These are the averages of data generated from over 4 days of the nightly runs using the OpenSearch Benchmark.&lt;/p&gt;

&lt;h4 id=&quot;highlights-1&quot;&gt;Highlights&lt;/h4&gt;

&lt;p&gt;Here are the key highlights:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An increase in performance with aggregate queries on workloads such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; workload, showcasing an improvement ranging between 50% to 70% over the default configuration.&lt;/li&gt;
  &lt;li&gt;The log analytics use cases for range queries demonstrated an improvement of around 65%.&lt;/li&gt;
  &lt;li&gt;Aggregation queries with hourly data aggregations, such as those for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hourly_agg&lt;/code&gt; workload, demonstrated a boost of up to 50% in performance.&lt;/li&gt;
  &lt;li&gt;Comparable latencies for auto or date histogram queries, with no noteworthy improvement or regression in performance.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multi_term_agg&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asc_sort_size&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dec_sort_size&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scroll&lt;/code&gt; queries showed regression. To delve deeper into the intricacies, the concurrent search contributors are proactively addressing this in the upcoming OpenSearch 2.12 GA release.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;detailed-results-opensearch-211-with-concurrent-search-enabled-vs-disabled&quot;&gt;Detailed results: OpenSearch 2.11 with concurrent search enabled vs. disabled&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: OpenSearch 2.11.0 single node (r5.2xlarge) with 64 GB RAM and 32 GB heap. Index settings: 1 Shard and 0 replicas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; workload results:&lt;/strong&gt; The following table illustrates a benchmark comparison of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nyc_taxis&lt;/code&gt; workload for OpenSearch 2.11 with concurrent search disabled and enabled (with 0 slices and with 4 slices). It includes the 90th percentile of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;took&lt;/code&gt; time latency measurements for each (p90) and the observed percentage improvements.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;CS disabled (p90_value)&lt;/th&gt;
        &lt;th&gt;CS enabled---0-slice (p90_value)&lt;/th&gt;
        &lt;th&gt;CS enabled---4-slice (p90_value)&lt;/th&gt;
        &lt;th&gt;% improvement (with 0 slices)&lt;/th&gt;
        &lt;th&gt;% improvement (with 4 slices)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;autohisto_agg&lt;/td&gt;
        &lt;td&gt;575&lt;/td&gt;
        &lt;td&gt;295&lt;/td&gt;
        &lt;td&gt;287&lt;/td&gt;
        &lt;td&gt;49%&lt;/td&gt;
        &lt;td&gt;50%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;date_histogram_agg&lt;/td&gt;
        &lt;td&gt;563&lt;/td&gt;
        &lt;td&gt;292&lt;/td&gt;
        &lt;td&gt;288&lt;/td&gt;
        &lt;td&gt;48%&lt;/td&gt;
        &lt;td&gt;49%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
        &lt;td&gt;17%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;distance_amount_agg&lt;/td&gt;
        &lt;td&gt;15,043&lt;/td&gt;
        &lt;td&gt;4,691&lt;/td&gt;
        &lt;td&gt;4744&lt;/td&gt;
        &lt;td&gt;69%&lt;/td&gt;
        &lt;td&gt;68%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;range&lt;/td&gt;
        &lt;td&gt;201&lt;/td&gt;
        &lt;td&gt;73&lt;/td&gt;
        &lt;td&gt;77&lt;/td&gt;
        &lt;td&gt;64%&lt;/td&gt;
        &lt;td&gt;62%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workload results:&lt;/strong&gt; The following table illustrates a benchmark comparison of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_logs&lt;/code&gt; workload for OpenSearch 2.11 with concurrent search disabled and enabled (with 0 slices and with 4 slices). It includes the 90th percentile of latency measurements for each (p90) and the observed percentage improvements.&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Operations&lt;/th&gt;
        &lt;th&gt;CS disabled (p90_value)&lt;/th&gt;
        &lt;th&gt;CS enabled---0-slice (p90_value)&lt;/th&gt;
        &lt;th&gt;CS enabled---4-slice (p90_value)&lt;/th&gt;
        &lt;th&gt;% improvement (with 0 slices)&lt;/th&gt;
        &lt;th&gt;% improvement (with 4 slices)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;200s-in-range&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;33%&lt;/td&gt;
        &lt;td&gt;33%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;400s-in-range&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;asc-sort-timestamp-after-force-merge-1-seg&lt;/td&gt;
        &lt;td&gt;20&lt;/td&gt;
        &lt;td&gt;20&lt;/td&gt;
        &lt;td&gt;22&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
        &lt;td&gt;-10%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;asc-sort-with-after-timestamp-after-force-merge-1-seg&lt;/td&gt;
        &lt;td&gt;85&lt;/td&gt;
        &lt;td&gt;86&lt;/td&gt;
        &lt;td&gt;86&lt;/td&gt;
        &lt;td&gt;-1%&lt;/td&gt;
        &lt;td&gt;-1%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;yellow-clr&quot;&gt;
        &lt;td&gt;asc_sort_size&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;-67%&lt;/td&gt;
        &lt;td&gt;-67%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;asc_sort_timestamp&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;asc_sort_with_after_timestamp&lt;/td&gt;
        &lt;td&gt;34&lt;/td&gt;
        &lt;td&gt;33&lt;/td&gt;
        &lt;td&gt;34&lt;/td&gt;
        &lt;td&gt;3%&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;default&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
        &lt;td&gt;25%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;desc-sort-timestamp-after-force-merge-1-seg&lt;/td&gt;
        &lt;td&gt;64&lt;/td&gt;
        &lt;td&gt;62&lt;/td&gt;
        &lt;td&gt;67&lt;/td&gt;
        &lt;td&gt;3%&lt;/td&gt;
        &lt;td&gt;-5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;desc-sort-with-after-timestamp-after-force-merge-1-seg&lt;/td&gt;
        &lt;td&gt;67&lt;/td&gt;
        &lt;td&gt;66&lt;/td&gt;
        &lt;td&gt;68&lt;/td&gt;
        &lt;td&gt;1%&lt;/td&gt;
        &lt;td&gt;-1%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;yellow-clr&quot;&gt;
        &lt;td&gt;desc_sort_size&lt;/td&gt;
        &lt;td&gt;6&lt;/td&gt;
        &lt;td&gt;91&lt;/td&gt;
        &lt;td&gt;9&lt;/td&gt;
        &lt;td&gt;-1417%&lt;/td&gt;
        &lt;td&gt;-50%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;yellow-clr&quot;&gt;
        &lt;td&gt;desc_sort_timestamp&lt;/td&gt;
        &lt;td&gt;26&lt;/td&gt;
        &lt;td&gt;34&lt;/td&gt;
        &lt;td&gt;28&lt;/td&gt;
        &lt;td&gt;-31%&lt;/td&gt;
        &lt;td&gt;-8%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;desc_sort_with_after_timestamp&lt;/td&gt;
        &lt;td&gt;63&lt;/td&gt;
        &lt;td&gt;61&lt;/td&gt;
        &lt;td&gt;63&lt;/td&gt;
        &lt;td&gt;3%&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;green-clr&quot;&gt;
        &lt;td&gt;hourly_agg&lt;/td&gt;
        &lt;td&gt;8180&lt;/td&gt;
        &lt;td&gt;3832&lt;/td&gt;
        &lt;td&gt;4034&lt;/td&gt;
        &lt;td&gt;53%&lt;/td&gt;
        &lt;td&gt;51%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;orange-clr&quot;&gt;
        &lt;td&gt;multi_term_agg&lt;/td&gt;
        &lt;td&gt;9818&lt;/td&gt;
        &lt;td&gt;40015&lt;/td&gt;
        &lt;td&gt;54107&lt;/td&gt;
        &lt;td&gt;-308%&lt;/td&gt;
        &lt;td&gt;-451%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;range&lt;/td&gt;
        &lt;td&gt;15&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
        &lt;td&gt;13&lt;/td&gt;
        &lt;td&gt;20%&lt;/td&gt;
        &lt;td&gt;13%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&quot;orange-clr&quot;&gt;
        &lt;td&gt;scroll&lt;/td&gt;
        &lt;td&gt;179&lt;/td&gt;
        &lt;td&gt;375&lt;/td&gt;
        &lt;td&gt;212&lt;/td&gt;
        &lt;td&gt;-109%&lt;/td&gt;
        &lt;td&gt;-18%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;term&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
        &lt;td&gt;0%&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;</content><author><name>sisurab</name></author><category term="technical-posts" /><category term="community" /><summary type="html"></summary></entry><entry><title type="html">OpenSearch research: Customer expectations of an intelligent dashboard assistant</title><link href="https://kolchfa-aws.github.io/project/blog/customer-expectations-of-an-intelligent-dashboard-assistant/" rel="alternate" type="text/html" title="OpenSearch research: Customer expectations of an intelligent dashboard assistant" /><published>2023-12-21T00:00:00+00:00</published><updated>2023-12-21T21:54:37+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/customer-expectations-of-an-intelligent-dashboard-assistant</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/customer-expectations-of-an-intelligent-dashboard-assistant/">&lt;p&gt;Given the increasing demand for artificial intelligence (AI), the obvious evolution of the dashboard user experience is the development of an intelligent assistant. User experience relies predominantly on the maturity of the design experience, which is based on six levels of development. As defined by Nielsen, this process of evolution delineates the presence of UX across several states: absent, limited, emergent, structured, integrated, and, finally, the most desired state of being user driven (s&lt;em&gt;ee&lt;/em&gt; &lt;a href=&quot;https://www.nngroup.com/articles/ux-maturity-model/&quot;&gt;&lt;em&gt;UX maturity covers processes, design, research, leadership support, and longevity of UX in product design&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;h4 id=&quot;key-considerations&quot;&gt;Key considerations&lt;/h4&gt;

&lt;p&gt;When a user consumes dashboard data, their end goal is to interpret data in a way that aids effective business decision-making. Motivations that drive the dashboard user’s UX preferences are often different from motivations that drive &lt;a href=&quot;https://opensearch.org/blog/q1-survey-results/&quot;&gt;dashboard producers&lt;/a&gt; (developers or data scientists who create dashboards) in setting up visualizations. Another useful context for dashboard consumption is whether the user accesses the visualization tool proactively (for monitoring, to establish compliance, or to set up a new platform) or reactively (in alerting use cases, in the event unusual activity needs to be investigated, or when data breaches occur). However, the most pressing concerns for log analysts are internal threats to data, social engineering, anomalies in security monitoring, triggered firewalls, and threat engineering. These situations require log and security analytics tools that engage users in a meaningful experience.&lt;/p&gt;

&lt;h4 id=&quot;terms-and-expectations&quot;&gt;Terms and expectations&lt;/h4&gt;

&lt;p&gt;Product managers and designers should consider user expectations in developing a dashboard solution that not only addresses the user’s unique business challenges but also provides them with a user experience that enables proper visualization of their data. When developing software, it is important to have an understanding of how tone, mode, customization, and personalization are perceived by users. Ultimately, when developing an AI assistant, understanding what contributes to user trust in an AI assistant is vital. Given that OpenSearch is used to power log analytics solutions created by developers (dashboard producers), we reached out to recruit them via our bi-monthly community meetings. We recruited seven developers who were also dashboard producers and two solutions architects. &lt;a href=&quot;https://graylog.registration.goldcast.io/events/7d8313a9-c8c1-4971-87ab-e3f59bcc6581?utm_campaign=Conference+Graylog+GO+2023&amp;amp;utm_source=Website&amp;amp;utm_medium=Large-eyebrow-banner#Registration&quot;&gt;We shared our findings at the recent Graylog GO 2023 user conference in Houston&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Producers expressed less interest in curating the interaction between an AI assistant and a user and focused more on ensuring user trust in AI results. They offered suggestions such as proxies, increasing user trust with accuracy metrics, displaying the data models used to produce results, and user ratings for commonly displayed results. Moreover, producers were more interested in how an AI assistant could help them.&lt;/p&gt;

&lt;p&gt;They wanted an AI assistant to help with queries by translating them to visualizations, providing links to the best documentation, offering suggestions on building better dashboards, offering insights on monitoring visualization and dashboard usage, and providing assistance with finding, structuring, and visualizing data. They also expressed the potential usefulness of an AI assistant in offering suggestions regarding the implementation of advanced analytics, such as alerting and anomaly detection. When pressed on their end user interaction and their role in curating the experience, producers took a hands-off approach. While they did express strong skepticism that their users would trust the AI assistant, they also offered overall feedback on methods of engendering user trust, such as visuals, boosting confidence in accuracy and relevance, and other user-guided customizations.&lt;/p&gt;

&lt;h4 id=&quot;talking-to-ai&quot;&gt;Talking to AI&lt;/h4&gt;

&lt;p&gt;&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/08.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/09.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/10.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/11.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/12.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/13.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given the results of our interviews with producers, we realized the importance of understanding how end users would determine the requirements of an AI assistant and how to best curate an experience for them. We assimilated results from 385 dashboard users. The tools they used included Kibana, Grafana, OpenSearch, Tableau, and QuickSight. We first analyzed feedback from log analytics users, 57.8% of whom were Kibana users. They indicated that they predominantly (42.1%) used logs and traces. When asked about what they would like to do with an AI assistant, all of them (100%) indicated that they would use an AI assistant in a query, and 57.8% of them indicated that they would use an AI assistant to find relevant documentation. For design and UX insights, we drew inferences from the larger sample.&lt;/p&gt;

&lt;h4 id=&quot;use-cases&quot;&gt;Use cases&lt;/h4&gt;

&lt;p&gt;When asked if they had used AI on a dashboard before, most users (97.9%) indicated that they had not. Around a third (32.2%) of dashboard users indicated that they had used dashboards to monitor metrics. Around a quarter (23.4%) indicated that they had used an AI assistant to answer a specific question. Other use cases included obtaining insights and recommendations on creating a dashboard or visualization, help with drilldowns and applying filters, help finding visualizations, dashboards, and data, and help interpreting findings and identifying patterns in data.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/14.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/15.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/17.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;visual-preferences&quot;&gt;Visual preferences&lt;/h4&gt;

&lt;p&gt;Most users (63.1%) wanted to invoke an AI assistant only when needed rather than it being in a persistent state. 37.9% wanted to invoke an assistant by selecting an icon or button, and 82.5% wanted to converse with an AI assistant in natural language. Dashboard users also wanted to use an AI assistant to ask for relevant documentation (73.7%) and to input a query (70.9%).&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/04.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/05.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/06.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/07.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/20.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/21.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Despite the fact that most survey responders had not used an AI assistant on a dashboard before, 83.6% of them expressed having a very robust mental model of where they expected to view an AI assistant. 42.8% expected it to be on the right-hand side of the screen, and 58.4% expected it to be on the left-hand side of the screen. When asked about the primacy of interface, more than half (62%) indicated that an AI assistant would be their secondary interface. 83.8% wanted an AI assistant to be in the form of a chat window, and 31.4% wanted it to be in the form of a search bar in which they could enter a question and receive an answer. When it came to language, users expected to converse with an AI assistant in a text-based, casual, natural, narrative format. As for tone, users expressed the need for a combination of formal and informal tones, indicating an area of design requiring further analysis.&lt;/p&gt;

&lt;p&gt;Trust in technology was a key theme for users of all types. Users communicated a need for accurate results. Moreover, users were hoping to personalize the tone, appearance, and other aspects of an AI assistant. Control in the form of naming the AI assistant and customizing the length of an answer, the location of the AI assistant, and the management of dashboard visualizations and metrics is important to consumers, and these appear to be expected features in an AI assistant. More research on what would reinforce trust, especially with the low-code user in terms of votes from other users, accuracy of results, underlying models used in producing results, and insights used to justify business and domain decisions, is needed.&lt;/p&gt;

&lt;h4 id=&quot;sample-information&quot;&gt;Sample information&lt;/h4&gt;

&lt;p&gt;&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/16.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/01.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/02.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/03.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/19.png&quot; /&gt;
&lt;img width=&quot;250&quot; src=&quot;/assets/media/blog-images/2023-12-21-customer-expectations-of-an-intelligent-dashboard-assistant/18.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We presented the findings of this research at the Graylog GO 2023 user conference.&lt;/p&gt;

&lt;!--
Copyright (c) 2020 Nathan Lam
https://github.com/nathancy/jekyll-embed-video
--&gt;

&lt;div class=&quot;embed-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/zBmEkTN7Jb8&quot; width=&quot; 640&quot; height=&quot;385&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; allow=&quot;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot;&gt;
    &lt;/iframe&gt;
  &lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://graylog.registration.goldcast.io/events/7d8313a9-c8c1-4971-87ab-e3f59bcc6581?utm_campaign=Conference+Graylog+GO+2023&amp;amp;utm_source=Website&amp;amp;utm_medium=Large-eyebrow-banner#Registration&quot;&gt;Graylog GO 2023&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Nielsen Norman Group, “&lt;a href=&quot;https://www.nngroup.com/articles/ux-maturity-model/&quot;&gt;The 6 Levels of UX Maturity&lt;/a&gt;”&lt;/li&gt;
  &lt;li&gt;Sundar, A. “&lt;a href=&quot;https://opensearch.org/blog/opensearch-dashboards-community-member-insights/&quot;&gt;OpenSearch Dashboards: Community Insights&lt;/a&gt;”&lt;/li&gt;
  &lt;li&gt;Sundar, A. “&lt;a href=&quot;https://opensearch.org/blog/q1-survey-results/&quot;&gt;OpenSearch Project Q1 community survey results&lt;/a&gt;”&lt;/li&gt;
&lt;/ol&gt;</content><author><name>apasun</name></author><category term="community" /><summary type="html">Given the increasing demand for artificial intelligence (AI), the obvious evolution of the dashboard user experience is the development of an intelligent assistant. User experience relies predominantly on the maturity of the design experience, which is based on six levels of development. As defined by Nielsen, this process of evolution delineates the presence of UX across several states: absent, limited, emergent, structured, integrated, and, finally, the most desired state of being user driven (s*ee* [*UX maturity covers processes, design, research, leadership support, and longevity of UX in product design*](https://www.nngroup.com/articles/ux-maturity-model/)).</summary></entry><entry><title type="html">Announcing the OpenSearch Project Leadership Committee</title><link href="https://kolchfa-aws.github.io/project/blog/announcing-opensearch-project-leadership-committee/" rel="alternate" type="text/html" title="Announcing the OpenSearch Project Leadership Committee" /><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-21T22:35:50+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/announcing-opensearch-project-leadership-committee</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/announcing-opensearch-project-leadership-committee/">&lt;p&gt;Earlier this year, the OpenSearch Project celebrated its two-year anniversary as an Apache 2.0–licensed project. Since its inception, the project has delivered 2 major and 14 minor releases, growing to over 1,000 contributors, including 200 maintainers spanning 17 organizations. In this time, AWS has acted as the steward of OpenSearch, opening the release process, hosting a Slack instance and forums, and creating public development and triage meetings. The OpenSearch Project recognizes that we can still do better in transparent decision-making, and we are taking another step on our journey toward a community-driven model of governance. We are thrilled to announce a sustaining OpenSearch Project Leadership Committee (LC) to help the OpenSearch community advance toward these goals.&lt;/p&gt;

&lt;p&gt;The goal of the LC is to preserve and advance the OpenSearch Project in order to benefit the OpenSearch community and represent its views. The OpenSearch community includes its maintainers, users, and contributing organizations. The LC, therefore, comprises of OpenSearch maintainers, repository owners, product developers, engineers, developer advocates, and employees of companies using OpenSearch in their products. It is a long-term focus group that will inform the governance of the open-source project, and its membership will be determined by the OpenSearch community. The initial members of the LC have each invested time and effort in developing OpenSearch in some way since the start of the project. We, Anandhi Bumstead (AWS), Mark Cohen (AWS), Eli Fisher (AWS), Kris Freedain (AWS), Charlotte Henkle (AWS), Samuel Herman (Oracle), Grant Ingersoll (Develomentor/Lucene), Nicholas Knize (AWS/Lucene), Jonah Kowall (Aiven/Cloud Native Computing Foundation), Andriy Redko (Aiven), Nithya Ruff (Amazon/Linux Foundation), Mehul A. Shah (Aryn.ai), and Amitai Stern (Logz.io), are happy to be involved as the initial members of the committee.&lt;/p&gt;

&lt;p&gt;OpenSearch needs more focus in two areas on which the LC will act. First, we will be intentional about the future of the project and the vision of where OpenSearch belongs for the long-term viability of it as an open-source project for the community and by the community. Second, we will outline and publish documentation on how project decisions are made, defining principles of development, and defining responsibilities of members and maintainers.&lt;/p&gt;

&lt;p&gt;We are passionate about open source and look forward to moving OpenSearch into a more open and transparent future. Please watch for additional communication from the LC in the coming weeks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-12-12-announcing-opensearch-project-leadership-committee/leadershipcommittee.png&quot; alt=&quot;OpenSearch Project Leadership Committee meeting&quot; class=&quot;img-fluid&quot; /&gt;&lt;/p&gt;</content><author><name>amistrn</name></author><category term="community" /><summary type="html">We are taking another step on our journey toward a community-driven model of governance. We are thrilled to announce a sustaining OpenSearch Project Leadership Committee (LC) to help the OpenSearch community advance toward these goals.</summary></entry><entry><title type="html">Semantic Search with OpenSearch: Architecture options and Benchmarks</title><link href="https://kolchfa-aws.github.io/project/blog/semantic-options-benchmarks/" rel="alternate" type="text/html" title="Semantic Search with OpenSearch: Architecture options and Benchmarks" /><published>2023-12-07T00:00:00+00:00</published><updated>2023-12-08T01:18:00+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/semantic-options-benchmarks</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/semantic-options-benchmarks/">&lt;p&gt;Unlike traditional lexical search algorithms such as BM25, which only take keywords into account, semantic search improves search relevance by understanding the context and semantic meaning of search terms and context. In general, semantic search has two key elements: 1. &lt;strong&gt;Embedding generation&lt;/strong&gt;: A machine learning (ML) model, usually a deep neural network model (for example, TAS-B) is used to generate embeddings for both search terms and content; 2. &lt;strong&gt;k-NN&lt;/strong&gt;: Searches return results based on embedding proximity using a vector search algorithm like k-nearest neighbors (k-NN).&lt;/p&gt;

&lt;p&gt;OpenSearch introduced the k-NN plugin to support vector search in 2019. However, users were left to manage embedding generation outside of OpenSearch. This changed with OpenSearch 2.9, when the new Neural Search plugin was released (available as an experimental feature in 2.4). The Neural Search plugin enables the integration of ML models into your search workloads. During ingestion and search, the plugin uses the ML model to transform text into vectors. Then it performs vector-based search using k-NN and returns semantically similar text-based search results.&lt;/p&gt;

&lt;p&gt;The addition of the vector transformation to the search process does come with a cost. It involves making inferences using deep neural network (DNN) language models, such as TAS-B. And the inferences of these DNN models are usually RAM and CPU heavy. If not set up correctly, it can result in resource consumption pressure and impact the health of your cluster. In the rest of this post, we’ll introduce several different ways of configuring OpenSearch clusters for semantic search, explain in detail how each approach works, and present a set of benchmarks to help you choose one to fit your own use case.&lt;/p&gt;

&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;

&lt;p&gt;Before we discuss the options, here are the definitions of some terms we’ll use throughout this post:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data node&lt;/strong&gt;: Where OpenSearch data is stored. A data node manages a cluster’s search and indexing tasks and is the primary coordinator of an OpenSearch cluster.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ML node&lt;/strong&gt;: OpenSearch introduced ML nodes in 2.3. An ML node is dedicated to ML-related tasks, such as inference for language models. You can follow these instructions to set up a dedicated ML node.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ML connector&lt;/strong&gt;: Introduced with ML extensibility in 2.9, an ML connector allows you to connect your preferred inference service (for example, Amazon SageMaker) to OpenSearch. Once created, an ML connector can be used to build an ML model, which is registered just like a local model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Local/remote inference&lt;/strong&gt;: With the newly introduced ML connector, OpenSearch allows ML inference to be hosted either locally on data or ML nodes; or remotely on public inference services.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture-options&quot;&gt;Architecture options&lt;/h2&gt;

&lt;p&gt;OpenSearch provides multiple options for enabling semantic search: 1. Local inference on data nodes, 2. Local inference on ML nodes, 3. Remote inference on data nodes, and 4. Remote inference on ML nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Option 1: Local inference on data nodes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With this option, both the Neural Search and ML Commons plugins reside on data nodes, just as any other plugin. Language models are loaded onto local data nodes, and inference is also executed locally.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ingest flow&lt;/strong&gt;: As illustrated in Figure 1, the Neural Search plugin receives ingestion requests through the ingestion pipeline. It sends the text blob to ML Commons to generate embeddings. ML Commons runs the inference locally and returns the generated embeddings. Neural Search then ingests the generated embeddings into a k-NN index.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Query flow&lt;/strong&gt;: For query requests, the Neural Search plugin also sends the query to ML Commons, which will inference locally and return an embedding. Upon receiving the embedding, Neural Search will create a vector search request and send it to the k-NN plugin, which will execute the query and return a list of document IDs. These document IDs will then be returned to the user.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-12-07-semantic-options-benchmarks/semantic-options-1.png&quot; alt=&quot;Figure 1: Local inference on data nodes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Option 2: Local inference on ML nodes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With this option, dedicated ML nodes are set up to perform all ML-related tasks, including inference for language models. Everything else is identical to option 1. In both the ingestion and query flows, the inference request to generate embeddings will now be sent to the ML Commons plugin, which resides on a dedicated ML node instead of on a data node, as shown in following figure:
&lt;img src=&quot;/assets/media/blog-images/2023-12-07-semantic-options-benchmarks/semantic-options-2.png&quot; alt=&quot;Figure 2: Local inference on ML nodes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Option 3: Remote inference on data nodes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This option was introduced in OpenSearch 2.9 with the ML extensibility feature. With this option, you use the ML connector to integrate with a remote server (outside of OpenSearch) for model inference (for example, SageMaker). Again, everything else is identical to option 1, except the inference requests are now forwarded by ML Commons from data nodes to the remote SageMaker endpoint through an ML connector, as shown in following figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-12-07-semantic-options-benchmarks/semantic-options-3.png&quot; alt=&quot;Figure 3: Remote inference on data nodes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Option 4: Remote inference on ML nodes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This option is a combination of options 2 and 3. It still uses remote inference from SageMaker but also uses a dedicated ML node to host ML Commons, as shown in following figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-12-07-semantic-options-benchmarks/semantic-options-4.png&quot; alt=&quot;Figure 4: Remote inference on ML nodes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each of the four options presents some pros and cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Option 1 is the default out-of-the-box option, it requires the least amount of setup configuration, and the inference requests are organically distributed by OpenSearch’s request routing. But running ML models on data nodes could potentially affect other data node tasks, such as querying and ingestion.&lt;/li&gt;
  &lt;li&gt;Option 2 manages all ML tasks with dedicated ML nodes. The benefit of this option is that it decouples the ML tasks from the rest of the cluster, improving the reliability of the cluster. But this also adds an extra network hop to ML nodes, which increases inference latency.&lt;/li&gt;
  &lt;li&gt;Option 3 leverages an existing inference service, such as SageMaker. The remote connection will introduce extra network latency, but it also provides the benefit of offloading resource-intensive tasks to a dedicated inference server, which improves the reliability of the cluster and offers more model serving flexibility.&lt;/li&gt;
  &lt;li&gt;Option 4 adds dedicated ML nodes on top of remote inference. Similarly to option 2, the dedicated ML node manages all ML requests, which further separates the ML workload from the rest of the cluster. But this comes with the cost of the ML node. Also, because the heavy lifting of the ML workload happens outside of the cluster, the ML node utilization could be low with this option.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;p&gt;To better understand the ingestion/query performance difference between these options, we designed a series of benchmarking tests. The following are our results and observations.&lt;/p&gt;

&lt;h3 id=&quot;experiment-setup&quot;&gt;Experiment setup&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Dataset: We used MS MARCO as the primary dataset for benchmarking. MS MARCO is a collection of datasets focused on deep learning in search. MS MARCO has 12M documents, with an average length of 1,500 words, and is approximately 100 GB in size. Also note that we have truncation set up in models to only use the first 128 tokens of each document in our experiments.&lt;/li&gt;
  &lt;li&gt;Model: We chose sentence-transformers/all-MiniLM-L6-v2 from a list of &lt;a href=&quot;https://opensearch.org/docs/latest/ml-commons-plugin/pretrained-models/#supported-pretrained-models&quot;&gt;pretrained models&lt;/a&gt; supported by OpenSearch.
    &lt;ol&gt;
      &lt;li&gt;All pretrained models support truncation/padding to control the input length; we set both at 128.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Cluster configuration:
    &lt;ol&gt;
      &lt;li&gt;Node type: M5.xlarge (4 core, 16 GB RAM)&lt;/li&gt;
      &lt;li&gt;To ensure an apples-to-apples comparison, we configured all cluster options to use the same type and number of nodes in order to keep the cost similar:
        &lt;ol&gt;
          &lt;li&gt;Option 1: Local inference on data nodes: 2 data nodes, 1 ML node&lt;/li&gt;
          &lt;li&gt;Option 2: Local inference on ML nodes: 3 data nodes&lt;/li&gt;
          &lt;li&gt;Option 3: Remote inference on data nodes: 2 data nodes, 1 SageMaker node&lt;/li&gt;
          &lt;li&gt;Option 4: Remote inference on ML nodes: 1 data node, 1 ML node, 1 SageMaker node&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Benchmarking tool: We used &lt;a href=&quot;https://github.com/opensearch-project/opensearch-benchmark&quot;&gt;OpenSearch Benchmark&lt;/a&gt; to generate traffic and collect results.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;experiment-1-ingestion&quot;&gt;Experiment 1: Ingestion&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Ingestion setup&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Configuration&lt;/th&gt;
      &lt;th&gt;Value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of clients&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bulk size&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Document count&lt;/td&gt;
      &lt;td&gt;1M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Local model truncation&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SageMaker model truncation&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Local model padding&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SageMaker model padding&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dataset&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/msmarco.zip&quot;&gt;MSCARCO&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Experiment 1: Results&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Case&lt;/th&gt;
      &lt;th&gt;Mean throughput (doc/s)&lt;/th&gt;
      &lt;th&gt;Inference p90 (ms/doc)&lt;/th&gt;
      &lt;th&gt;SageMaker inference p90 (ms/req)&lt;/th&gt;
      &lt;th&gt;SageMaker overhead p90 (ms/req)&lt;/th&gt;
      &lt;th&gt;e2e latency p90 (ms/bulk)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Option 1: Local inference on data nodes (3 data nodes)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;&lt;em&gt;213.13&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;72.46&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;8944.53&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Option 2: Local inference on ML nodes (2 data nodes + 1 ML node)&lt;/td&gt;
      &lt;td&gt;72.76&lt;/td&gt;
      &lt;td&gt;67.79&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;25936.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Option 3: Remote inference on data nodes (2 data nodes + 1 remote ML node)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;&lt;em&gt;94.41&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;101.9&lt;/td&gt;
      &lt;td&gt;97&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;17455.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Option 4: Remote inference on ML nodes (1 data node + 1 local ML node + 1 remote ML node)&lt;/td&gt;
      &lt;td&gt;79.79&lt;/td&gt;
      &lt;td&gt;60.37&lt;/td&gt;
      &lt;td&gt;54.8&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;21714.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Experiment 1: Observations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Option 1 provides much higher throughput than the other options. This is probably because ML models were deployed to all three data nodes, while the other options have only one dedicated ML node performing inference work. Note that we didn’t perform other tasks during the experiment, so all the nodes are dedicated to ingestion. This might not be the case in a real-world scenario. When the cluster multitasks, the ML inference workload may impact other tasks and cluster health.&lt;/li&gt;
  &lt;li&gt;Comparing options 2 and 3, we can see that even though option 2 has lower inference latency, its throughput is much lower than with option 3, which has a remote ML node. This could be because the SageMaker node is built and optimized solely for inference, while the local ML node still runs the OpenSearch stack and is not optimized for an inference workload.&lt;/li&gt;
  &lt;li&gt;Remote inference added some trivial overhead (3.5 ms, SageMaker overhead). We ran our tests on a public network; testing run on a virtual private cloud (VPC)-based network might yield slightly different results, but they are unlikely to be significant.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiment-2-query&quot;&gt;Experiment 2: Query&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Query setup&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Configuration&lt;/th&gt;
      &lt;th&gt;Value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of clients&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Document count&lt;/td&gt;
      &lt;td&gt;500k&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Local model truncation&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SageMaker model truncation&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Local model padding&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SageMaker model padding&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dataset&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/msmarco.zip&quot;&gt;MSCARCO&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Experiment 2: Results&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Case&lt;/th&gt;
      &lt;th&gt;Mean throughput (query/s)&lt;/th&gt;
      &lt;th&gt;Inference p90 (ms/query)&lt;/th&gt;
      &lt;th&gt;SageMaker inference p90 (ms/req)&lt;/th&gt;
      &lt;th&gt;SageMaker overhead p90	(ms/req)&lt;/th&gt;
      &lt;th&gt;e2e latency p90 (ms/query)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Option 1: Local inference on data nodes (3 data nodes)&lt;/td&gt;
      &lt;td&gt;128.49&lt;/td&gt;
      &lt;td&gt;37.6&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;82.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Option 2: Local inference on ML nodes (2 data nodes + 1 ML node)&lt;/td&gt;
      &lt;td&gt;141.5&lt;/td&gt;
      &lt;td&gt;29.5&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;72.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Option 3: Remote inference on data nodes (2 data nodes + remote ML node)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;&lt;em&gt;162.19&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;26.4&lt;/td&gt;
      &lt;td&gt;21.5&lt;/td&gt;
      &lt;td&gt;4.9&lt;/td&gt;
      &lt;td&gt;72.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Option 4: Remote inference on ML nodes (1 data node + 1 local ML node + remote ML node)&lt;/td&gt;
      &lt;td&gt;136.2&lt;/td&gt;
      &lt;td&gt;26.6&lt;/td&gt;
      &lt;td&gt;21.6&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;76.65&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Experiment 2: Observations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inference latency is much lower than in the ingestion experiment (~30 ms compared to 60–100 ms). This is primarily because query terms are usually much shorter than documents.&lt;/li&gt;
  &lt;li&gt;Externally hosted models outperformed local models on inference tasks by about 10%, even considering the network overhead.&lt;/li&gt;
  &lt;li&gt;Unlike with ingestion, inference latency is a considerable part of end-to-end query latency. So the configuration that has the lowest latency achieves higher throughput and lower end-to-end latency.&lt;/li&gt;
  &lt;li&gt;The remote model with a dedicated ML node ranked lowest in throughput, which could be because all remote requests have to pass through the single ML node instead of through multiple data nodes, as in the other configurations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusionrecommendations&quot;&gt;Conclusion/Recommendations&lt;/h2&gt;

&lt;p&gt;In this blog post, we provided multiple options for configuring your OpenSearch cluster for semantic search, including local/remote inference and dedicated ML nodes. You can choose between these options to optimize costs and benefits based on your desired outcome. Based on our benchmarking results and observations, we recommend the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Remotely connected models separate ML workloads from the OpenSearch cluster, with only a small amount of extra latency. This option also provides flexibility in terms of the amount of computation power used for making inferences (for example, leveraging SageMaker GPU instances). This is our &lt;strong&gt;recommended&lt;/strong&gt; option for any production-oriented systems.&lt;/li&gt;
  &lt;li&gt;Local inference works out of the box on existing clusters without any additional resources. You can use this option to quickly set up a development environment or build PoCs. Because the heavy ML workload could potentially affect cluster query and search performance, we don’t recommend this option for production systems. If you do have to use local inference for your production systems, we strongly recommend to use dedicated ML nodes to separate ML workload from the rest of you cluster.&lt;/li&gt;
  &lt;li&gt;Dedicated ML nodes helps improve query latency for local models (by taking over all ML-related tasks from data nodes), but they don’t help much with remote inference because the heavy lifting is performed outside of the OpenSearch cluster. Also, because ML nodes don’t manage any tasks not related to ML, adding an ML node won’t improve query or ingestion throughput.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seanzheng</name></author><category term="technical-post" /><summary type="html">Unlike traditional lexical search algorithms such as BM25, which only take keywords into account, semantic search improves search relevance by understanding the context and semantic meaning of search terms and context. In general, semantic search has two key elements: 1. Embedding generation: A machine learning (ML) model, usually a deep neural network model (for example, TAS-B) is used to generate embeddings for both search terms and content; 2. k-NN: Searches return results based on embedding proximity using a vector search algorithm like k-nearest neighbors (k-NN).</summary></entry><entry><title type="html">Improving document retrieval with sparse semantic encoders</title><link href="https://kolchfa-aws.github.io/project/blog/improving-document-retrieval-with-sparse-semantic-encoders/" rel="alternate" type="text/html" title="Improving document retrieval with sparse semantic encoders" /><published>2023-12-05T08:00:00+00:00</published><updated>2023-12-08T16:49:59+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/improving-document-retrieval-with-sparse-semantic-encoders</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/improving-document-retrieval-with-sparse-semantic-encoders/">&lt;p&gt;OpenSearch 2.11 introduced neural sparse search—a new efficient method of semantic retrieval. In this blog post, you’ll learn about using sparse encoders for semantic search. You’ll find that neural sparse search reduces costs, performs faster, and improves search relevance. We’re excited to share benchmarking results and show how neural sparse search outperforms other search methods. You can even try it out by building your own search engine in just five steps. To skip straight to the results, see &lt;a href=&quot;#benchmarking-results&quot;&gt;Benchmarking results&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-are-dense-and-sparse-vector-embeddings&quot;&gt;What are dense and sparse vector embeddings?&lt;/h2&gt;

&lt;p&gt;When you use a transformer-based encoder, such as BERT, to generate traditional dense vector embeddings, the encoder translates each word into a vector. Collectively, these vectors make up a semantic vector space. In this space, the closer the vectors are, the more similar the words are in meaning.&lt;/p&gt;

&lt;p&gt;In sparse encoding, the encoder uses the text to create a list of tokens that have similar semantic meaning. The model vocabulary (&lt;a href=&quot;https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt&quot;&gt;WordPiece&lt;/a&gt;) contains most commonly used words along with various tense endings (for example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-ed&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-ing&lt;/code&gt;) and suffixes (for example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-ate&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-ion&lt;/code&gt;). You can think of the vocabulary as a semantic space where each document is a sparse vector.&lt;/p&gt;

&lt;p&gt;The following images show example results of dense and sparse encoding.&lt;/p&gt;

&lt;table style=&quot;border:none&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;border:none&quot;&gt;
        &lt;img height=&quot;280px&quot; src=&quot;/assets/media/blog-images/2023-12-05-improving-document-retrieval-with-spade-semantic-encoders/embedding.png&quot; /&gt;
    &lt;/td&gt;
    &lt;td style=&quot;border:none&quot;&gt;
        &lt;img height=&quot;280px&quot; src=&quot;/assets/media/blog-images/2023-12-05-improving-document-retrieval-with-spade-semantic-encoders/expand.png&quot; /&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Left&lt;/strong&gt;: Dense vector semantic space. &lt;strong&gt;Right&lt;/strong&gt;: Sparse vector semantic space.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;sparse-encoders-use-more-efficient-data-structures&quot;&gt;Sparse encoders use more efficient data structures&lt;/h2&gt;

&lt;p&gt;In dense encoding, documents are represented as high-dimensional vectors. To search these documents, you need to use a k-NN index as an underlying data structure. In contrast, sparse search can use a native Lucene index because sparse encodings are similar to term vectors used by keyword-based matching.&lt;/p&gt;

&lt;p&gt;Compared to k-NN indexes, &lt;strong&gt;sparse embeddings have the following cost-reducing advantages&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Much smaller index size&lt;/li&gt;
  &lt;li&gt;Reduced runtime RAM cost&lt;/li&gt;
  &lt;li&gt;Lower computational cost&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For a detailed comparison, see &lt;a href=&quot;#table-ii-speed-comparison-in-terms-of-latency-and-throughput&quot;&gt;Table II&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;sparse-encoders-perform-better-on-unfamiliar-datasets&quot;&gt;Sparse encoders perform better on unfamiliar datasets&lt;/h2&gt;

&lt;p&gt;In our previous &lt;a href=&quot;https://opensearch.org/blog/semantic-science-benchmarks&quot;&gt;blog post&lt;/a&gt;, we mentioned that searching with dense embeddings presents challenges when encoders encounter unfamiliar content. When an encoder trained on one dataset is used on a different dataset, the encoder often produces unpredictable embeddings, resulting in poor search result relevance.&lt;/p&gt;

&lt;p&gt;Often, BM25 performs better than dense encoders on BEIR datasets that incorporate strong domain knowledge. In these cases, sparse encoders can fall back on keyword-based matching, ensuring that their search results are no worse than those produced by BM25. For a comparison of search result relevance benchmarks, see &lt;a href=&quot;#table-i-relevance-comparison-on-beir-benchmark-and-amazon-esci-in-terms-of-ndcg10-and-rank&quot;&gt;Table I&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;among-sparse-encoders-document-only-encoders-are-the-most-efficient&quot;&gt;Among sparse encoders, document-only encoders are the most efficient&lt;/h2&gt;

&lt;p&gt;You can run a neural sparse search in two modes: &lt;strong&gt;bi-encoder&lt;/strong&gt; and &lt;strong&gt;document-only&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In bi-encoder mode, both documents and search queries are passed through deep encoders. In document-only mode, documents are still passed through deep encoders, but search queries are instead tokenized. In this mode, document encoders are trained to learn more synonym association in order to increase recall. By eliminating the online inference phase, you can &lt;strong&gt;save computational resources&lt;/strong&gt; and &lt;strong&gt;significantly reduce latency&lt;/strong&gt;. For benchmarks, compare the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Neural sparse document-only&lt;/code&gt; column with the other columns in &lt;a href=&quot;#table-ii-speed-comparison-in-terms-of-latency-and-throughput&quot;&gt;Table II&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;neural-sparse-search-outperforms-other-search-methods-in-benchmarking-tests&quot;&gt;Neural sparse search outperforms other search methods in benchmarking tests&lt;/h2&gt;

&lt;p&gt;For benchmarking, we used a cluster containing 3 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r5.8xlarge&lt;/code&gt; data nodes and 1 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r5.12xlarge&lt;/code&gt; leader/machine learning (ML) node. We measured search relevance for all evaluated search methods in terms of NCDG@10. Additionally, we compared the runtime speed and the resource cost of each method.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Here are the key takeaways:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both modes provide the highest relevance on the BEIR and Amazon ESCI datasets.&lt;/li&gt;
  &lt;li&gt;Without online inference, the search latency of document-only mode is comparable to BM25.&lt;/li&gt;
  &lt;li&gt;Sparse encoding results in a much smaller index size than dense encoding. A document-only sparse encoder generates an index that is &lt;strong&gt;10.4%&lt;/strong&gt; of the size of a dense encoding index. For a bi-encoder, the index size is &lt;strong&gt;7.2%&lt;/strong&gt; of the size of a dense encoding index.&lt;/li&gt;
  &lt;li&gt;Dense encoding uses k-NN retrieval and incurs a 7.9% increase in RAM cost at search time. Neural sparse search uses a native Lucene index, so the RAM cost does not increase at search time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;benchmarking-results&quot;&gt;Benchmarking results&lt;/h2&gt;

&lt;p&gt;The benchmarking results are presented in the following tables.&lt;/p&gt;

&lt;h3 id=&quot;table-i-relevance-comparison-on-beir-benchmark-and-amazon-esci-in-terms-of-ndcg10-and-rank&quot;&gt;Table I. Relevance comparison on BEIR benchmark and Amazon ESCI in terms of NDCG@10 and rank&lt;/h3&gt;

&lt;table&gt;
    &lt;tr style=&quot;text\-align:center;&quot;&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td colspan=&quot;2&quot;&gt;BM25&lt;/td&gt;
        &lt;td colspan=&quot;2&quot;&gt;Dense (with TAS-B model)&lt;/td&gt;
        &lt;td colspan=&quot;2&quot;&gt;Hybrid (Dense + BM25)&lt;/td&gt;
        &lt;td colspan=&quot;2&quot;&gt;Neural sparse search bi-encoder&lt;/td&gt;
        &lt;td colspan=&quot;2&quot;&gt;Neural sparse search document-only&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;b&gt;Dataset&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;NDCG&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;Rank&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;NDCG&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;Rank&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;NDCG&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;Rank&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;NDCG&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;Rank&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;NDCG&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;Rank&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Trec-Covid&lt;/td&gt;
        &lt;td&gt;0.688&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.481&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.698&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.771&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.707&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;NFCorpus&lt;/td&gt;
        &lt;td&gt;0.327&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.319&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.335&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.36&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.352&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;NQ&lt;/td&gt;
        &lt;td&gt;0.326&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.463&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.418&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.553&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.521&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;HotpotQA&lt;/td&gt;
        &lt;td&gt;0.602&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.579&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.636&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.697&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.677&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;FiQA&lt;/td&gt;
        &lt;td&gt;0.254&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.3&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.322&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.376&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.344&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ArguAna&lt;/td&gt;
        &lt;td&gt;0.472&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;0.427&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.378&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.508&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.461&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Touche&lt;/td&gt;
        &lt;td&gt;0.347&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.162&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.313&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;0.278&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.294&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;DBPedia&lt;/td&gt;
        &lt;td&gt;0.287&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.383&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.387&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.447&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.412&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;SciDocs&lt;/td&gt;
        &lt;td&gt;0.165&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;0.149&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.174&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.164&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.154&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;FEVER&lt;/td&gt;
        &lt;td&gt;0.649&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.697&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.77&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;0.821&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.743&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Climate FEVER&lt;/td&gt;
        &lt;td&gt;0.186&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.228&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.251&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;0.263&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.202&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;SciFact&lt;/td&gt;
        &lt;td&gt;0.69&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.643&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.672&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.723&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.716&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Quora&lt;/td&gt;
        &lt;td&gt;0.789&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.835&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.864&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0.856&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;0.788&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Amazon ESCI&lt;/td&gt;
        &lt;td&gt;0.081&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
        &lt;td&gt;0.071&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0.086&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;0.077&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;0.095&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Average&lt;/td&gt;
        &lt;td&gt;0.419&lt;/td&gt;
        &lt;td&gt;3.71&lt;/td&gt;
        &lt;td&gt;0.41&lt;/td&gt;
        &lt;td&gt;4.29&lt;/td&gt;
        &lt;td&gt;0.45&lt;/td&gt;
        &lt;td&gt;2.71&lt;/td&gt;
        &lt;td&gt;0.492&lt;/td&gt;
        &lt;td&gt;1.64&lt;/td&gt;
        &lt;td&gt;0.462&lt;/td&gt;
        &lt;td&gt;2.64&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; For more information about Benchmarking Information Retrieval (BEIR), see &lt;a href=&quot;https://github.com/beir-cellar/beir&quot;&gt;the BEIR GitHub page&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;table-ii-speed-comparison-in-terms-of-latency-and-throughput&quot;&gt;Table II. Speed comparison in terms of latency and throughput&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;BM25&lt;/th&gt;
      &lt;th&gt;Dense (with TAS-B model)&lt;/th&gt;
      &lt;th&gt;Neural sparse search bi-encoder&lt;/th&gt;
      &lt;th&gt;Neural sparse search document-only&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;P50 latency (ms)&lt;/td&gt;
      &lt;td&gt;8 ms&lt;/td&gt;
      &lt;td&gt;56.6 ms&lt;/td&gt;
      &lt;td&gt;176.3 ms&lt;/td&gt;
      &lt;td&gt;10.2ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;P90 latency (ms)&lt;/td&gt;
      &lt;td&gt;12.4 ms&lt;/td&gt;
      &lt;td&gt;71.12 ms&lt;/td&gt;
      &lt;td&gt;267.3 ms&lt;/td&gt;
      &lt;td&gt;15.2ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;P99 Latency (ms)&lt;/td&gt;
      &lt;td&gt;18.9 ms&lt;/td&gt;
      &lt;td&gt;86.8 ms&lt;/td&gt;
      &lt;td&gt;383.5 ms&lt;/td&gt;
      &lt;td&gt;22ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max throughput (op/s)&lt;/td&gt;
      &lt;td&gt;2215.8 op/s&lt;/td&gt;
      &lt;td&gt;318.5 op/s&lt;/td&gt;
      &lt;td&gt;107.4 op/s&lt;/td&gt;
      &lt;td&gt;1797.9 op/s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mean throughput (op/s)&lt;/td&gt;
      &lt;td&gt;2214.6 op/s&lt;/td&gt;
      &lt;td&gt;298.2 op/s&lt;/td&gt;
      &lt;td&gt;106.3 op/s&lt;/td&gt;
      &lt;td&gt;1790.2 op/s&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; We tested latency on a subset of MS MARCO v2 containing 1M documents in total. To obtain latency data, we used 20 clients to loop search requests.&lt;/p&gt;

&lt;h3 id=&quot;table-iii-resource-consumption-comparison&quot;&gt;Table III. Resource consumption comparison&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;BM25&lt;/th&gt;
      &lt;th&gt;Dense (with TAS-B model)&lt;/th&gt;
      &lt;th&gt;Neural sparse search bi-encoder&lt;/th&gt;
      &lt;th&gt;Neural sparse search document-only&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Index size&lt;/td&gt;
      &lt;td&gt;1 GB&lt;/td&gt;
      &lt;td&gt;65.4 GB&lt;/td&gt;
      &lt;td&gt;4.7 GB&lt;/td&gt;
      &lt;td&gt;6.8 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RAM usage&lt;/td&gt;
      &lt;td&gt;480.74 GB&lt;/td&gt;
      &lt;td&gt;675.36 GB&lt;/td&gt;
      &lt;td&gt;480.64 GB&lt;/td&gt;
      &lt;td&gt;494.25 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Runtime RAM delta&lt;/td&gt;
      &lt;td&gt;+0.01 GB&lt;/td&gt;
      &lt;td&gt;+53.34 GB&lt;/td&gt;
      &lt;td&gt;+0.06 GB&lt;/td&gt;
      &lt;td&gt;+0.03 GB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; We performed this experiment using the full MS MARCO v2 dataset, containing 8.8M passages. For all methods, we excluded the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_source&lt;/code&gt; fields and force merged the index before measuring index size. We set the heap size of the OpenSearch JVM to half of the node RAM, so an empty OpenSearch cluster still consumed close to 480 GB of memory.&lt;/p&gt;

&lt;h2 id=&quot;build-your-search-engine-in-five-steps&quot;&gt;Build your search engine in five steps&lt;/h2&gt;

&lt;p&gt;Follow these steps to build your search engine:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: For this simple setup, update the following cluster settings:&lt;/p&gt;

    &lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;PUT&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/_cluster/settings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;transient&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;plugins.ml_commons.allow_registering_model_via_url&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;plugins.ml_commons.only_run_on_ml_node&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;plugins.ml_commons.native_memory_threshold&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;For more information about ML-related cluster settings, see &lt;a href=&quot;https://opensearch.org/docs/latest/ml-commons-plugin/cluster-settings/&quot;&gt;ML Commons cluster settings&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Deploy encoders&lt;/strong&gt;: The ML Commons plugin supports deploying pretrained models using a URL. For this example, you’ll deploy the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;opensearch-neural-sparse-encoding&lt;/code&gt; encoder:&lt;/p&gt;

    &lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/_plugins/_ml/models/_register?deploy=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;opensearch-neural-sparse-encoding&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1.0.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;description&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;opensearch-neural-sparse-encoding&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;model_format&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;TORCH_SCRIPT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;function_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;SPARSE_ENCODING&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;model_content_hash_value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;d1ebaa26615090bdb0195a62b180afd2a8524c68c5d406a11ad787267f515ea8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;url&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://artifacts.opensearch.org/models/ml-models/amazon/neural-sparse/opensearch-neural-sparse-encoding-v1/1.0.1/torch_script/neural-sparse_opensearch-neural-sparse-encoding-v1-1.0.1-torch_script.zip&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;OpenSearch responds with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task_id&lt;/code&gt;:&lt;/p&gt;

    &lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;task_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;task_id&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;status&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CREATED&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task_id&lt;/code&gt; to check the status of the task:&lt;/p&gt;

    &lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;GET&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/_plugins/_ml/tasks/&amp;lt;task_id&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Once the task is complete, the task state changes to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;COMPLETED&lt;/code&gt; and OpenSearch returns the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_id&lt;/code&gt; for the deployed model:&lt;/p&gt;

    &lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;model_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;model_id&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;task_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;REGISTER_MODEL&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;function_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;SPARSE_TOKENIZE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;COMPLETED&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;worker_node&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;wubXZX7xTIC7RW2z8nzhzw&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;create_time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1701390988405&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;last_update_time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1701390993724&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;is_async&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Set up ingestion&lt;/strong&gt;: In OpenSearch, a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sparse_encoding&lt;/code&gt; ingest processor encodes documents into sparse vectors before indexing them. Create an ingest pipeline as follows:&lt;/p&gt;

    &lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;PUT&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/_ingest/pipeline/neural-sparse-pipeline&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;description&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;An example neural sparse encoding pipeline&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;processors&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;sparse_encoding&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                 &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;model_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;model_id&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                 &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;field_map&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;passage_text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;passage_embedding&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Set up index mapping&lt;/strong&gt;: Neural search uses the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rank_features&lt;/code&gt; field type to store token weights when documents are indexed. The index will use the ingest pipeline you created to generate text embeddings. Create the index as follows:&lt;/p&gt;

    &lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;PUT&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/my-neural-sparse-index&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;settings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;default_pipeline&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;neural-sparse-pipeline&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;mappings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;properties&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;passage_embedding&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                 &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rank_features&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;passage_text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                 &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ingest documents using the ingest pipeline&lt;/strong&gt;: After creating the index, you can ingest documents into it. When you index a text field, the ingest processor converts text into a vector embedding and stores it in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;passage_embedding&lt;/code&gt; field specified in the processor:&lt;/p&gt;

    &lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;PUT&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/my-neural-sparse-index/_doc/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;passage_text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hello world&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Try your engine with a query clause&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Congratulations! You’ve now created your own semantic search engine based on sparse encoders. To try a sample query, invoke the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_search&lt;/code&gt; endpoint using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neural_sparse&lt;/code&gt; query:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;GET&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/my-neural-sparse-index/_search/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;query&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;neural_sparse&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;passage_embedding&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;query_text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hello world a b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;model_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;model_id&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;max_token_score&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;neural-sparse-query-parameters&quot;&gt;Neural sparse query parameters&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neural_sparse&lt;/code&gt; query supports two parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_id&lt;/code&gt; (String): The ID of the model that is used to generate tokens and weights from the query text. A sparse encoding model will expand the tokens from query text, while the tokenizer model will only tokenize the query text itself.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_token_score&lt;/code&gt; (Float): An extra parameter required for performance optimization. Just like a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;match&lt;/code&gt; query, a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neural_sparse&lt;/code&gt; query is transformed to a Lucene BooleanQuery, combining term-level subqueries using disjunction. The difference is that a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neural_sparse&lt;/code&gt; query uses FeatureQuery instead of TermQuery to match the terms. Lucene employs the Weak AND (WAND) algorithm for dynamic pruning, which skips non-competitive tokens based on their score upper bounds. However, FeatureQuery uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FLOAT.MAX_VALUE&lt;/code&gt; as the score upper bound, which makes the WAND optimization ineffective. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_token_score&lt;/code&gt; parameter resets the score upper bound for each token in a query, which is consistent with the original FeatureQuery. Thus, setting the value to 3.5 for the bi-encoder model and to 2 for the document-only model can accelerate search without precision loss. After OpenSearch is upgraded to Lucene version 9.8, this parameter will be deprecated.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;selecting-a-model&quot;&gt;Selecting a model&lt;/h2&gt;

&lt;p&gt;OpenSearch provides several pretrained encoder models that you can use out of the box without fine-tuning. For a list of sparse encoding models provided by OpenSearch, see &lt;a href=&quot;https://opensearch.org/docs/latest/ml-commons-plugin/pretrained-models/#sparse-encoding-models&quot;&gt;Sparse encoding models&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Use the following recommendations to select a sparse encoding model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For &lt;strong&gt;bi-encoder&lt;/strong&gt; mode, we recommend using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;opensearch-neural-sparse-encoding-v1&lt;/code&gt; pretrained model. For this model, both online search and offline ingestion share the same model file.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For &lt;strong&gt;document-only&lt;/strong&gt; mode, we recommended using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;opensearch-neural-sparse-encoding-doc-v1&lt;/code&gt; pretrained model for ingestion and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;opensearch-neural-sparse-tokenizer-v1&lt;/code&gt; model at search time to implement online query tokenization. This model does not employ model inference and only translates the query into tokens.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;For more information about neural sparse search, see &lt;a href=&quot;https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/&quot;&gt;Neural sparse search&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;For an end-to-end neural search tutorial, see &lt;a href=&quot;https://opensearch.org/docs/latest/search-plugins/neural-search-tutorial/&quot;&gt;Neural search tutorial&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;For a list of all search methods OpenSearch supports, see &lt;a href=&quot;https://opensearch.org/docs/latest/search-plugins/index/#search-methods&quot;&gt;Search methods&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Provide your feedback on the &lt;a href=&quot;https://forum.opensearch.org/&quot;&gt;OpenSearch Forum&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>zhichaog</name></author><category term="technical-posts" /><summary type="html">OpenSearch 2.11 introduced neural sparse search—a new efficient method of semantic retrieval. In this blog post, you’ll learn about using sparse encoders for semantic search. You’ll find that neural sparse search reduces costs, performs faster, and improves search relevance. We’re excited to share benchmarking results and show how neural sparse search outperforms other search methods. You can even try it out by building your own search engine in just five steps. To skip straight to the results, see Benchmarking results.</summary></entry><entry><title type="html">OpenSearch adds a new generative AI assistant toolkit</title><link href="https://kolchfa-aws.github.io/project/blog/opensearch-adds-new-generative-ai-assistant-toolkit/" rel="alternate" type="text/html" title="OpenSearch adds a new generative AI assistant toolkit" /><published>2023-11-30T18:30:00+00:00</published><updated>2023-12-01T23:41:57+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/opensearch-adds-new-generative-ai-assistant-toolkit</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/opensearch-adds-new-generative-ai-assistant-toolkit/">&lt;p&gt;OpenSearch is widely used by developers to build search and analytics solutions. One of the areas top of mind for our community has been how to use transformative technologies like artificial intelligence (AI) and machine learning (ML) to improve their search and analytics applications. OpenSearch has supported vector database capabilities, like k-NN, since its inception. Over the last year we have continued to innovate by adding new AI/ML features, like conversational search, neural search, and ML model extensibility through &lt;a href=&quot;https://opensearch.org/docs/latest/ml-commons-plugin/index/&quot;&gt;ML Commons&lt;/a&gt;, to the OpenSearch suite of capabilities. Putting these capabilities in place has allowed us to innovate further by incorporating generative AI into OpenSearch.&lt;/p&gt;

&lt;p&gt;Generative AI is changing how users interact with and derive insights from their data. We are excited to share that OpenSearch has released the OpenSearch Assistant, a toolkit designed to provide OpenSearch developers with flexible and customizable tools for building generative AI experiences. By integrating natural language processing and contextual awareness features, this toolkit helps developers use large language models (LLMs) to deliver smart, unique, and interactive user experiences that unlock actionable insights from complex datasets. This is the beginning of the OpenSearch Project’s generative AI journey. We are committed to delivering seamless AI and ML innovations in OpenSearch, helping the community to unlock the potential of generative AI capabilities.&lt;/p&gt;

&lt;p&gt;We invite you to explore the capabilities of the OpenSearch Assistant and imagine how it can be used to solve your search and analytics challenges. Watch the following video to see an example experience built using the OpenSearch Assistant.&lt;/p&gt;

&lt;!--
Copyright (c) 2020 Nathan Lam
https://github.com/nathancy/jekyll-embed-video
--&gt;

&lt;div class=&quot;embed-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/9r0RyB_oHKk&quot; width=&quot; 640&quot; height=&quot;385&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; allow=&quot;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot;&gt;
    &lt;/iframe&gt;
  &lt;/div&gt;

&lt;h3 id=&quot;how-does-it-work&quot;&gt;How does it work?&lt;/h3&gt;

&lt;p&gt;The OpenSearch Assistant toolkit comprises a few key building blocks that help developers create AI-powered assistants inside of OpenSearch Dashboards. These building blocks include &lt;em&gt;skills&lt;/em&gt; that define and automate various tasks performed by the assistant, the &lt;em&gt;ML framework&lt;/em&gt; that allows OpenSearch to connect the assistant to an LLM or a fine-tuned model, and &lt;em&gt;UI components&lt;/em&gt; for building interactive conversation-based experiences.&lt;/p&gt;

&lt;p&gt;A skill is a lightweight task that the assistant runs for a user. For example, if a user enters “Show me all of my indices”, a single skill would be used to list all of the indices in a cluster. Multiple skills can be combined to answer complex questions, such as “Show me the active alerts on my largest index.” In this case, a skill would first list the indices to find the largest index. Another skill would then list the alert monitors for that index to find the corresponding alerts. This effectively combines two different skills to answer a single question.&lt;/p&gt;

&lt;p&gt;We have included a variety of foundational skills, such as &lt;em&gt;CAT indices&lt;/em&gt;, which lists all indices; &lt;em&gt;Piped Processing Language (PPL) query generation&lt;/em&gt;, which can convert user-provided natural language prompts into a PPL query; and many more. PPL is a query language that lets you use pipe ( &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|&lt;/code&gt; ) syntax to explore, discover, and query data in OpenSearch. These skills are available as part of code repositories for &lt;a href=&quot;https://github.com/opensearch-project/dashboards-assistant/tree/feature/langchain&quot;&gt;LangChain&lt;/a&gt;, &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch-Dashboards/tree/feature/2.11/os-assistant&quot;&gt;Dashboards&lt;/a&gt;, and the &lt;a href=&quot;https://github.com/opensearch-project/dashboards-observability/tree/feature/explorer-query-assistant&quot;&gt;query assistant&lt;/a&gt;. While the current skills make use of LangChain, the project is working toward incorporating core ML skills that can be conveniently accessed by any OpenSearch feature. If you are interested in learning more about future plans and contributing to the evolution of the OpenSearch Assistant skills, please review and comment on &lt;a href=&quot;https://github.com/opensearch-project/dashboards-assistant/issues/18&quot;&gt;this RFC&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The ML framework underlying this assistant integrates AI models with OpenSearch Assistant using no-code connectors. It simplifies interactions with AI technologies, making them more accessible and user friendly.&lt;/p&gt;

&lt;p&gt;To help developers build impactful user experiences, the toolkit also incorporates several UI components for OpenSearch Dashboards. These components include chat interaction patterns, feature interactions, and a conversational search bar.&lt;/p&gt;

&lt;h3 id=&quot;see-it-in-action-with-our-query-assistant-demo&quot;&gt;See it in action with our query assistant demo&lt;/h3&gt;

&lt;p&gt;Imagine using natural language conversations to get answers from your data. Using our new query assistant demo, you can experience the ease of querying your OpenSearch logs. This demo experience is designed to automate and simplify user interactions using natural language prompts. The assistant automatically converts user prompts to PPL queries for the specified data and then summarizes the results in a simple response. The assistant combines the three building blocks we previously mentioned: skills, the ML framework, and UI components. It eliminates the need for deep technical expertise, making observability data insights readily accessible to users of all skill levels. Let us dive into how we built this demo experience using a selected set of skills from the toolkit.&lt;/p&gt;

&lt;p&gt;First, we used a few essential skills to construct the workflows. One of these skills is the “PPL Query Generation” skill, which can transform a natural language question into a PPL query. For instance, if you say “Are there any errors in my logs?”, this skill would translate that into the PPL query &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source=opensearch_dashboards_sample_data_logs | where QUERY_STRING([&apos;response&apos;], &apos;4* OR 5*&apos;)&lt;/code&gt;. This query specifically looks for 4xx and 5xx HTTP response codes in the logs. Additionally, we used the “Summarization” skill to convert the query results into a concise summary for quick analysis. If an error occurs, then the “Suggestion” skill is automatically engaged, which can not only explain the issue but also suggest alternative questions for consideration.&lt;/p&gt;

&lt;p&gt;Next, we connected these skills to an LLM to help generate the summary from the query results. The toolkit comes with fine-tuned prompts and connectors for the &lt;a href=&quot;https://www.anthropic.com/index/releasing-claude-instant-1-2&quot;&gt;Anthropic Claude Instant model&lt;/a&gt;. While this example is integrated with Anthropic Claude Instant model, the &lt;a href=&quot;https://opensearch.org/docs/latest/ml-commons-plugin/extensibility/index/&quot;&gt;ML framework&lt;/a&gt; allows users to integrate with a wider range of LLMs or fine-tuned models specific to their use case.&lt;/p&gt;

&lt;p&gt;Last, we needed to provide an intuitive and interactive OpenSearch Dashboards user experience. This was as simple as embedding the provided search bar UI component into our log explorer interface. The following diagram provides an architectural view of how these parts are used together.&lt;/p&gt;

&lt;p&gt;This example experience showcases one of many possible solutions you can build with the OpenSearch Assistant toolkit. We welcome you to try it out by simply logging in to &lt;a href=&quot;https://playground.opensearch.org/&quot;&gt;OpenSearch Playground&lt;/a&gt; and asking questions. If you’d like to check out the code, it’s all available on GitHub within these repos: &lt;a href=&quot;https://github.com/opensearch-project/dashboards-assistant/tree/feature/langchain&quot;&gt;LangChain&lt;/a&gt;, &lt;a href=&quot;https://github.com/opensearch-project/OpenSearch-Dashboards/tree/feature/2.11/os-assistant&quot;&gt;Dashboards&lt;/a&gt;, and the &lt;a href=&quot;https://github.com/opensearch-project/dashboards-observability/tree/feature/explorer-query-assistant&quot;&gt;query assistant&lt;/a&gt;. We’ve also built Docker images to help you stand up this demo with your own data by following &lt;a href=&quot;https://github.com/opensearch-project/dashboards-assistant/blob/main/GETTING_STARTED_GUIDE.md&quot;&gt;these steps&lt;/a&gt;. And we’ve created the following video demonstration of the kinds of AI-powered experiences developers can build with the OpenSearch Assistant toolkit.&lt;/p&gt;

&lt;!--
Copyright (c) 2020 Nathan Lam
https://github.com/nathancy/jekyll-embed-video
--&gt;

&lt;div class=&quot;embed-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/VTiJtGI2Sr4&quot; width=&quot; 640&quot; height=&quot;385&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; allow=&quot;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot;&gt;
    &lt;/iframe&gt;
  &lt;/div&gt;

&lt;h3 id=&quot;whats-next-for-the-opensearch-assistant&quot;&gt;What’s next for the OpenSearch Assistant?&lt;/h3&gt;

&lt;p&gt;This is just the beginning of the OpenSearch Assistant journey, and we look forward to working with you to make OpenSearch a delightful solution for building generative AI–powered applications. As next steps, we will make this toolkit more flexible and customizable and allow you to rebrand it to meet your needs. If you are interested in contributing or need more information, you can refer to &lt;a href=&quot;https://github.com/opensearch-project/dashboards-assistant/issues/18&quot;&gt;this RFC&lt;/a&gt;. If you’d like to share feedback, please use &lt;a href=&quot;https://forum.opensearch.org/t/feedback-opensearch-assistant/16741&quot;&gt;this OpenSearch forum thread&lt;/a&gt; or the OpenSearch &lt;a href=&quot;https://opensearch.slack.com/channels/assistant-feedback&quot;&gt;#assistant-feedback&lt;/a&gt; Slack channel.&lt;/p&gt;</content><author><name>jimishsh</name></author><category term="releases" /><summary type="html">OpenSearch is widely used by developers to build search and analytics solutions. One of the areas top of mind for our community has been how to use transformative technologies like artificial intelligence (AI) and machine learning (ML) to improve their search and analytics applications. OpenSearch has supported vector database capabilities, like k-NN, since its inception. Over the last year we have continued to innovate by adding new AI/ML features, like conversational search, neural search, and ML model extensibility through ML Commons, to the OpenSearch suite of capabilities. Putting these capabilities in place has allowed us to innovate further by incorporating generative AI into OpenSearch.</summary></entry><entry><title type="html">Announcing Data Prepper 2.6.0</title><link href="https://kolchfa-aws.github.io/project/blog/Announcing-Data-Prepper-2.6.0/" rel="alternate" type="text/html" title="Announcing Data Prepper 2.6.0" /><published>2023-11-28T19:30:00+00:00</published><updated>2023-11-28T17:49:30+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/Announcing-Data-Prepper-2.6.0</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/Announcing-Data-Prepper-2.6.0/">&lt;p&gt;Data Prepper 2.6.0 is now available for &lt;a href=&quot;https://opensearch.org/downloads.html#data-prepper&quot;&gt;download&lt;/a&gt;.
Now you can now ingest data from DynamoDB, improve data durability by using the new Kafka buffer, and automatically connect to Amazon OpenSearch Serverless collections.&lt;/p&gt;

&lt;h2 id=&quot;dynamodb-source&quot;&gt;DynamoDB source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/dynamodb/&quot;&gt;Amazon DynamoDB&lt;/a&gt; is a high-scale, high-performance key-value database.
Generally, developers will query DynamoDB on the primary index and secondary indexes.
However, many teams would also like to search and analyze data in DynamoDB.
Now you can use Data Prepper and OpenSearch to search and analyze DynamoDB data.&lt;/p&gt;

&lt;p&gt;Data Prepper’s new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dynamodb&lt;/code&gt; source ingests items from a DynamoDB table so that you can index those items in OpenSearch.
You can import existing data that was backed up by DynamoDB’s point-in-time recovery.
For new data, Data Prepper can read from DynamoDB Streams to keep your OpenSearch cluster’s data up-to-date with DynamoDB.&lt;/p&gt;

&lt;p&gt;The feature supports change data capture (CDC), so it will keep the OpenSearch index up-to-date with DynamoDB.
You can add items, update items, and delete them.
Data Prepper will handle the complicated work of moving this data for you.&lt;/p&gt;

&lt;h2 id=&quot;kafka-buffer&quot;&gt;Kafka buffer&lt;/h2&gt;

&lt;p&gt;Data Prepper provides &lt;a href=&quot;/blog/End-to-end-acknowledgements-in-Data-Prepper&quot;&gt;end-to-end acknowledgements&lt;/a&gt; to ensure that data from pull-based sources reaches OpenSearch.
For push-based sources, Data Prepper currently has an in-memory buffer, but there is some risk of losing data when the node crashes.
For these sources, we can improve durability by storing data in an external system instead of locally on the Data Prepper node.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; is an open-source event streaming platform.
It is highly durable and can store events for as long as you configure them.
This makes it a great choice for durable storage of events in Data Prepper.&lt;/p&gt;

&lt;p&gt;Data Prepper now has a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka&lt;/code&gt; buffer type that uses Kafka to store data in flight.
You can use this feature to send data directly to Data Prepper and hold the data in Kafka before Data Prepper saves it to OpenSearch.&lt;/p&gt;

&lt;p&gt;Now existing clients such as the &lt;a href=&quot;https://opentelemetry.io/docs/collector/&quot;&gt;OpenTelemetry Collector&lt;/a&gt; and &lt;a href=&quot;https://fluentbit.io/&quot;&gt;Fluent Bit&lt;/a&gt; can send data to Data Prepper just as they do now, but with better durability.
You can abstract the internals of how you store data in Data Prepper and won’t need to change those client configurations.&lt;/p&gt;

&lt;p&gt;Additionally, Data Prepper’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka&lt;/code&gt; buffer supports per-event encryption so that you can perform client-side encryption if needed.&lt;/p&gt;

&lt;h2 id=&quot;amazon-opensearch-serverless-improvements&quot;&gt;Amazon OpenSearch Serverless improvements&lt;/h2&gt;

&lt;p&gt;Data Prepper improves integration with &lt;a href=&quot;https://aws.amazon.com/opensearch-service/features/serverless/&quot;&gt;Amazon OpenSearch Serverless&lt;/a&gt; with new options to update the network policy.
With this feature, you can configure Data Prepper to create an OpenSearch Serverless network policy to your VPC-based collections.
This simplifies some of the setup for developers who have the necessary permissions to create this policy.
This new configuration is available for both the OpenSearch sink and source.&lt;/p&gt;

&lt;h2 id=&quot;other-features&quot;&gt;Other features&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Data Prepper’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s3&lt;/code&gt; source provides duplication protection by extending the visibility timeout for Amazon Simple Queue Service (SQS) messages. We encourage users to add the necessary permissions and use this feature to avoid data duplication.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;opensearch&lt;/code&gt; source now allows for configuring a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;distribution_version&lt;/code&gt; to connect with ElasticSearch 7 clusters.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting started&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;To download Data Prepper, see the &lt;a href=&quot;https://opensearch.org/downloads.html&quot;&gt;OpenSearch downloads&lt;/a&gt; page.&lt;/li&gt;
  &lt;li&gt;For instructions on how to get started with Data Prepper, see &lt;a href=&quot;https://opensearch.org/docs/latest/data-prepper/getting-started/&quot;&gt;Getting started with Data Prepper&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;To learn more about the work in progress for Data Prepper 2.7, see the &lt;a href=&quot;https://github.com/opensearch-project/data-prepper/projects/1&quot;&gt;Data Prepper roadmap&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;thanks-to-our-contributors&quot;&gt;Thanks to our contributors!&lt;/h2&gt;

&lt;p&gt;The following people contributed to this release. Thank you!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/asuresh8&quot;&gt;asuresh8&lt;/a&gt; - Adi Suresh&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/asifsmohammed&quot;&gt;asifsmohammed&lt;/a&gt; - Asif Sohail Mohammed&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chenqi0805&quot;&gt;chenqi0805&lt;/a&gt; - Qi Chen&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/daixba&quot;&gt;daixba&lt;/a&gt; - Aiden Dai&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dinujoh&quot;&gt;dinujoh&lt;/a&gt; - Dinu John&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dlvenable&quot;&gt;dlvenable&lt;/a&gt; - David Venable&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/engechas&quot;&gt;engechas&lt;/a&gt; - Chase Engelbrecht&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/graytaylor0&quot;&gt;graytaylor0&lt;/a&gt; - Taylor Gray&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/hshardeesi&quot;&gt;hshardeesi&lt;/a&gt; – Hardeep Singh&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/KarstenSchnitter&quot;&gt;KarstenSchnitter&lt;/a&gt; - Karsten Schnitter&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kkondaka&quot;&gt;kkondaka&lt;/a&gt; - Krishna Kondaka&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mallikagogoi7&quot;&gt;mallikagogoi7&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/oeyh&quot;&gt;oeyh&lt;/a&gt; - Hai Yan&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Periecle&quot;&gt;Periecle&lt;/a&gt; - Roman Kvasnytskyi&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/reta&quot;&gt;reta&lt;/a&gt; - Andriy Redko&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wanghd89&quot;&gt;wanghd89&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>dlv</name></author><category term="releases" /><summary type="html">Data Prepper 2.6.0 is now available for download. Now you can now ingest data from DynamoDB, improve data durability by using the new Kafka buffer, and automatically connect to Amazon OpenSearch Serverless collections.</summary></entry><entry><title type="html">OpenSearch Sessions at re:Invent</title><link href="https://kolchfa-aws.github.io/project/blog/sessions-at-reinvent/" rel="alternate" type="text/html" title="OpenSearch Sessions at re:Invent" /><published>2023-11-24T00:00:00+00:00</published><updated>2023-11-24T19:38:12+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/sessions-at-reinvent</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/sessions-at-reinvent/">&lt;p&gt;&lt;a href=&quot;https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=OpenSearch&quot;&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-24-sessions-at-reinvent/reInvent_banner.jpg&quot; alt=&quot;OpenSearch Sessions at re:Invent Banner&quot; class=&quot;img-fluid&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With re:Invent right around the corner, I want to share a few of the OpenSearch sessions that are going to be particularly interesting. Some sessions pertain to open-source OpenSearch or the Amazon OpenSearch Service, but both should be applicable no matter how you run OpenSearch.&lt;/p&gt;

&lt;h1 id=&quot;1-opn311---extending-opensearch&quot;&gt;1. OPN311 - Extending OpenSearch&lt;/h1&gt;

&lt;p&gt;Time: Nov. 27, 10:00 AM - 11:00 AM (PST)
&lt;br /&gt;Location: &lt;a href=&quot;https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=OPN311&quot;&gt;Wynn, Level 1, Palmer 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Stephen Crawford and Daniel Doubrovkine (or DB) dive into how you can extend OpenSearch. If you are looking to create an extension, register a step for a search or ingestion pipeline, or are building your own plugin, this chalk talk will help you get started!&lt;/p&gt;

&lt;h1 id=&quot;2-ant312---using-amazon-opensearch-service-as-a-vector-database-for-gen-ai-apps&quot;&gt;2. ANT312 - Using Amazon OpenSearch Service as a vector database for gen AI apps&lt;/h1&gt;

&lt;p&gt;Time: Nov. 30, 11:30 AM - 1:30 PM (PST)
&lt;br /&gt;Location: &lt;a href=&quot;https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=ANT312&quot;&gt;Caesars Forum, Level 1, Forum 118&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If all you want to talk about is Generative AI, this session has you covered.  Arun Lakshmanan and Jianwei Li walk you through how to use OpenSearch as a context store for your Generative AI applications and what tools OpenSearch provides to build a comprehensive solution with Generative AI.&lt;/p&gt;

&lt;h1 id=&quot;3-opn202---logging-with-amazon-eks-bottlerocket-fluent-bit-and-opensearch&quot;&gt;3. OPN202 - Logging with Amazon EKS, Bottlerocket, Fluent Bit, and OpenSearch&lt;/h1&gt;

&lt;p&gt;Time: Nov. 29, 3:00 PM - 5:00 PM (PST)
&lt;br /&gt;Location: &lt;a href=&quot;https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=OPN202&quot;&gt;Venetian, Level 3, Murano 3203&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am excited to be running this end-to-end workshop with Kyle Davis, Senior Developer Advocate for Bottlerocket. In this workshop, we’ll cover how you can perform observability on a shell-less cluster. To start, we’ll deploy an EKS cluster onto nodes running Bottlerocket, a security hardened container focused operating system. Then, we’ll deploy OpenSearch and Fluent Bit to do observability on our cluster. No prior experience is required so we hope to see you there!&lt;/p&gt;

&lt;h1 id=&quot;4-ant210---improve-your-search-with-vector-capabilities-in-opensearch-service&quot;&gt;4. ANT210 - Improve your search with vector capabilities in OpenSearch Service&lt;/h1&gt;

&lt;p&gt;Time: Nov. 30, 2:30 PM - 3:30 PM (PST)
&lt;br /&gt;Location: &lt;a href=&quot;https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=ANT210&quot;&gt;Caesars Forum, Level 1, Summit 201&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Did you know that OpenSearch is not just a vector database but three vector databases in one? With OpenSearch, you can leverage vector stores like FAISS, nmslib, and Lucene’s Vector Store. In this session, Achal Kumar from Intuit along with Aruna Govindaraju and Jon Handler from AWS talk through how to get more relevant search results with OpenSearch as a vector store. Additionally, you’ll hear first hand how Intuit is using Amazon OpenSearch Service to get more relevant results.&lt;/p&gt;

&lt;h1 id=&quot;5-com310---processing-over-450-million-events-per-day-in-real-time-with-aws&quot;&gt;5. COM310 - Processing over 450 million events per day in real time with AWS&lt;/h1&gt;

&lt;p&gt;Time: Nov. 27, 5:30 PM - 5:50 PM (PST)
&lt;br /&gt;Location: &lt;a href=&quot;https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=COM310&quot;&gt;Venetian, Level 2, Hall B, Expo, Booth #1480, Theater 3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As an observability solution, OpenSearch scales! Hear from Neylson Crepalde at A3Data about how they are doing observability on a whopping 5Tb per month of logs. He’ll cover the architecture he used and the many lesson they learned along the way.&lt;/p&gt;

&lt;h1 id=&quot;6-ant301---whats-new-in-amazon-opensearch-service&quot;&gt;6. ANT301 - What’s new in Amazon OpenSearch Service&lt;/h1&gt;

&lt;p&gt;Time: Nov. 30, 4:00 PM - 5:00 PM (PST)
&lt;br /&gt;Locations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=ANT301&quot;&gt;Caesars Forum, Level 1, Summit 201&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=ANT301-SC1&quot;&gt;Mandalay Bay, Level 2, South, Mandalay Bay Ballroom L, Content Hub, Teal Screen&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hub.reinvent.awsevents.com/attendee-portal/catalog/?search=ANT301-SC2&quot;&gt;MGM Grand, Level 1, Boulevard 163, Content Hub, Yellow Screen&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And saving the best for last; make sure you checkout live what is new in the Amazon OpenSearch Service! This session hosted by Mukul (GM of Search Services at AWS), Carl (Director of OpenSearch at AWS), and Bill Burket (Engineering Leader at EA), covers many of the new things that are going to make your experience building on OpenSearch even better! You are not going to want to miss this session.&lt;/p&gt;

&lt;h1 id=&quot;other-things-to-see&quot;&gt;Other things to see&lt;/h1&gt;

&lt;p&gt;While those are some ways to learn from those using and building OpenSearch there are several other ways to connect as well! You will be able to find many experts at The OpenSearch Project booth (#105) and in the Modern Apps booth (#1390). Can’t wait to catch up with everyone there and if you’d like to chat about OpenSearch feel free to reach out to me on our &lt;a href=&quot;https://opensearch.org/slack.html&quot;&gt;public slack&lt;/a&gt;!&lt;/p&gt;</content><author><name>dtaivpp</name></author><category term="news" /><summary type="html">Wondering what sessions to attend at re:Invent to learn more about OpenSearch? We&apos;ve got you covered! Here are the top 6 sessions to attend this year at re:Invent to deepen your OpenSearch knowledge.</summary></entry><entry><title type="html">Building custom threat detection rules with OpenSearch Security Analytics</title><link href="https://kolchfa-aws.github.io/project/blog/how-to-create-custom-threat-detection-rules/" rel="alternate" type="text/html" title="Building custom threat detection rules with OpenSearch Security Analytics" /><published>2023-11-21T00:00:00+00:00</published><updated>2023-11-21T19:27:01+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/how-to-create-custom-threat-detection-rules</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/how-to-create-custom-threat-detection-rules/">&lt;p&gt;OpenSearch Security Analytics provides new threat monitoring, detection, and alerting features. These capabilities help you to detect and investigate potential security threats that may disrupt your business operations or pose a threat to sensitive organizational data. Security Analytics can simplify and increase the efficiency of your security operations by using its threat detection engine to find potential threats in real time. Security Analytics includes a collection of prepackaged detection rules, which you can tailor to your specific security requirements. The detection rules scan log data to produce security findings representing potential threats, visualizing them on a dashboard that includes details like severity, category, and tags. Additionally, you have the flexibility to create new detection rules and customize them for your log sources.&lt;/p&gt;

&lt;p&gt;Real-time threat detection requires a deep knowledge of security threat attack patterns and specialized investigation skills. Creating customized queries and rules to detect threats is a time-consuming task that can take hours or sometimes days. Security Analytics provides more than 2,200 prepackaged, open-source &lt;a href=&quot;https://github.com/SigmaHQ/sigma&quot;&gt;Sigma rules&lt;/a&gt; to help you identify potential security threats from a variety of log sources, including Microsoft Windows, AWS CloudTrail, Amazon S3 access logs, and many more. This saves users the time-consuming task of defining rules to detect common and emerging threats. In addition, Security Analytics provides a customizable framework that enables you to create custom detection rules for specific use cases or custom log sources.&lt;/p&gt;

&lt;p&gt;In this blog we will show you how to create custom detection rule for monitoring your log data for contacts to a specific set of domains. You will learn how to take advantage of the granularity and the precision in defining the threat signals that custom rules can provide.&lt;/p&gt;

&lt;p&gt;Before diving deep, it is essential to understand the basic concepts of security analytics such as detectors, findings, detection rules, and alerts. If you aren’t familiar with these terms, please refer to the &lt;a href=&quot;https://opensearch.org/docs/latest/security-analytics/index/&quot;&gt;documentation&lt;/a&gt; before proceeding.&lt;/p&gt;

&lt;h3 id=&quot;why-use-custom-detection-rules&quot;&gt;Why use custom detection rules?&lt;/h3&gt;

&lt;p&gt;Prepackaged detection rules are helpful in detecting known threats in real time from a diverse set of log sources. Often, users have business-specific applications, such as a fraud detection application, that generate nonstandard logs. To address the users’ need to monitor custom applications, Security Analytics lets you create custom detection rules and include them in detectors that monitor custom application logs for malicious attack signatures. Custom detection rules open up possibilities to employ all other security analytics capabilities, including the correlation engine, security findings, and alerts, to detect and investigate threats from virtually any log source.&lt;/p&gt;

&lt;p&gt;The common use cases for using custom detection rules include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Monitoring your data against a known list of values, such as emails or IP address ranges, that could be “in the list” and “not in list”. For example, an alert can be triggered when someone deletes one of 10 different security groups.&lt;/li&gt;
  &lt;li&gt;Reducing false positives in threat detection by using exceptions for a specific field. For example, an alert can be triggered for every user that is not created by a specific person from a specific index.&lt;/li&gt;
  &lt;li&gt;Generating alerts on the detected threats according to wildcard exceptions. For example, an alert can be triggered only if someone creates a new DNS record with a hostname from one particular domain, and not another domain.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the remainder of this blog, we guide you through the process of creating custom detection rules and providing filter criteria to help you narrow down on specific entities, ensuring you’re alerted when a potential threat associated with those entities is detected. For purposes of illustration, we will create a custom detection rule that enables the user to detect potentially malicious contacts from the organization email addresses.&lt;/p&gt;

&lt;h3 id=&quot;how-to-create-a-custom-detection-rule&quot;&gt;How to create a custom detection rule?&lt;/h3&gt;

&lt;p&gt;Let’s consider an example in which you need to monitor audit logs from Microsoft 365 for contacts with emails belonging to specific domain names and create alerts based on those findings.&lt;/p&gt;

&lt;p&gt;In this example we will create a detection rule with the criteria as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Monitor email logs of user-sent emails&lt;/li&gt;
  &lt;li&gt;Include a filter criteria that checks for external recipient email address domain&lt;/li&gt;
  &lt;li&gt;Exclude a list of users (marketing team) who are known to send emails to external addresses.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following are the step-by-step instructions to create a detection rule to be used as part of a custom detector:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://opensearch.org/docs/latest/quickstart/&quot;&gt;To start, log in to OpenSearch Dashboards&lt;/a&gt;, navigate to the Security Analytics plugin, and then continue to drill down to the &lt;strong&gt;Detection rules&lt;/strong&gt; page.&lt;/li&gt;
  &lt;li&gt;Select &lt;strong&gt;Create detection rule&lt;/strong&gt; to open a page showing parameters of your custom rule.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To create custom detection rules using the API, refer to the &lt;a href=&quot;https://opensearch.org/docs/latest/security-analytics/api-tools/rule-api/&quot;&gt;Rule APIs documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The detection rules are stored in &lt;a href=&quot;https://github.com/SigmaHQ/sigma/wiki/Rule-Creation-Guide&quot;&gt;Sigma rule format&lt;/a&gt;, and the workflow covers the fields that are aligned with the terminology adopted by the Sigma community.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Enter the rule meta information in all the required &lt;strong&gt;Rule overview&lt;/strong&gt; fields.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-21-how-to-create-custom-threat-detection-rules/rule-overview.png&quot; alt=&quot;Detection rule overview section&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;In the &lt;strong&gt;Details&lt;/strong&gt; section, select a log type, rule severity, and rule status.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-21-how-to-create-custom-threat-detection-rules/details.png&quot; alt=&quot;Rule details section&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Complete &lt;strong&gt;Selection_1&lt;/strong&gt; in the &lt;strong&gt;Detection&lt;/strong&gt; section to define the criteria used for the rule.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can add multiple &lt;strong&gt;Selections&lt;/strong&gt; with multiple maps for the log fields. &lt;strong&gt;Map&lt;/strong&gt; is a directory that contains key-value pairs. &lt;strong&gt;Modifiers&lt;/strong&gt; transform values and lists or convert them into regular expressions. To learn more about the Sigma rule format, go to the &lt;a href=&quot;https://github.com/SigmaHQ/sigma&quot;&gt;Sigma documentation&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a Map with &lt;strong&gt;Key&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UserID&lt;/code&gt;, &lt;strong&gt;Modifier&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;contains&lt;/code&gt;, and &lt;strong&gt;Value&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;your organization domain name&amp;gt;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Create a Map with &lt;strong&gt;Key&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Recipient&lt;/code&gt;, &lt;strong&gt;Modifier&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;endswith&lt;/code&gt;, and &lt;strong&gt;List&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;values that include the domains to be detected&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-21-how-to-create-custom-threat-detection-rules/selection1.png&quot; alt=&quot;Selection_1 section&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create another &lt;strong&gt;Selection&lt;/strong&gt; where &lt;strong&gt;Key&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UserId&lt;/code&gt; is a list of emails belonging to the marketing team by uploading the list from a .csv (or .txt) file.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-21-how-to-create-custom-threat-detection-rules/upload-a-file.png&quot; alt=&quot;Upload a file dialog&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After uploading the list, &lt;strong&gt;Selection_2&lt;/strong&gt; will look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-21-how-to-create-custom-threat-detection-rules/selection2.png&quot; alt=&quot;Selection_2 section&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Enter  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Selection_1 AND NOT include Selection_2&lt;/code&gt; in the &lt;strong&gt;Condition&lt;/strong&gt; fields to define how each &lt;strong&gt;Selection&lt;/strong&gt; will contribute to the final query.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The user interface (UI) allows for the combining of multiple selections using operators, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AND&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AND NOT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OR&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OR NOT&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pro tip&lt;/strong&gt;: If you’re more comfortable using YAML formats, you can switch to the YAML editor at the top of the page under the page title at any point and write a &lt;strong&gt;Condition&lt;/strong&gt; as code without losing the context.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-21-how-to-create-custom-threat-detection-rules/condition.png&quot; alt=&quot;Condition section&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once a custom rule is added, it will show up on the &lt;strong&gt;Detection rules&lt;/strong&gt; page and can be used in any new or existing detector for customized fine-grained threat detection.&lt;/p&gt;

&lt;h3 id=&quot;what-are-the-benefits&quot;&gt;What are the benefits?&lt;/h3&gt;

&lt;p&gt;A custom approach to detection rules that are tailored to your unique business cases helps target specific malicious activity across a range of cloud environments and troubleshoot application-specific threats. OpenSearch Security Analytics provides the capability to configure alerts and customize notification messages, helping you to find threats that need further investigation. By using the custom detection rules workflow, you can expand the detection criteria to include custom application log sources and monitor them for specific threat patterns. You can also use correlation rules to find patterns between standard and custom application logs.&lt;/p&gt;

&lt;p&gt;Integrating custom rules to the threat detection strategies empowers your security teams to enhance coverage for detecting security threats across a variety of log sources. Custom rules can be written with a deep understanding of an organization’s infrastructure, making them more precise in identifying specific threats and reducing false positives. Combining custom and prepackaged rules for threat detection strikes a balance between tailored and standardized protection.&lt;/p&gt;

&lt;p&gt;To get started on creating a custom detection rule for a custom log source or application log, refer to &lt;a href=&quot;https://opensearch.org/docs/latest/security-analytics/usage/rules/#creating-detection-rules&quot;&gt;documentation&lt;/a&gt;. To learn more about security analytics and its capabilities, see the &lt;a href=&quot;https://opensearch.org/docs/latest/security-analytics/index/&quot;&gt;Working with detection rules documentation&lt;/a&gt;.&lt;/p&gt;</content><author><name>xeniatup</name></author><category term="feature" /><category term="technical-posts" /><summary type="html">The threat detection rules scan log data to produce security findings representing potential threats. Security Analytics provides more than 2,200 prepackaged, open-source Sigma rules to help you identify potential security threats from a variety of log sources, including Microsoft Windows, AWS CloudTrail, Amazon S3 access logs, and many more. Additionally, you have the flexibility to create new detection rules and customize them for your log sources. In this blog we will show you how to create custom detection rules using an example in which you need to monitor audit logs from Microsoft 365 for contacts with emails belonging to specific domain names and create alerts based on those findings.</summary></entry><entry><title type="html">Secrets to improving ingestion with OpenSearch</title><link href="https://kolchfa-aws.github.io/project/blog/unlocking-the-secrets-to-ingestion/" rel="alternate" type="text/html" title="Secrets to improving ingestion with OpenSearch" /><published>2023-11-20T08:00:00+00:00</published><updated>2023-11-22T14:01:20+00:00</updated><id>https://kolchfa-aws.github.io/project/blog/unlocking-the-secrets-to-ingestion</id><content type="html" xml:base="https://kolchfa-aws.github.io/project/blog/unlocking-the-secrets-to-ingestion/">&lt;p&gt;While most of the secrets I’m sharing with you aren’t intentionally hidden, they sure felt hidden to me when I started with OpenSearch. Part of the reason they aren’t as well known is OpenSearch is growing at a rapid pace. Several things we will cover today were only released within the last few versions. Let’s take a look at how you can get the most out of your OpenSearch cluster, covering everything from your client library to how your data is persisted on disk. Using these strategies I was able to ingest data &lt;strong&gt;65% faster&lt;/strong&gt; and &lt;strong&gt;decrease the size on disk by 19%&lt;/strong&gt; (measured on a 3-node c5.xlarge cluster using the tarball installation).&lt;/p&gt;

&lt;h1 id=&quot;client-optimization&quot;&gt;Client Optimization&lt;/h1&gt;

&lt;p&gt;First and foremost, it’s important to understand our &lt;a href=&quot;https://opensearch.org/docs/latest/api-reference/document-apis/bulk/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_bulk&lt;/code&gt; API&lt;/a&gt;. The bulk endpoint allows users to send multiple document actions at once to OpenSearch. This is much more efficient then sending individual document updates one at a time. This is because the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http&lt;/code&gt; protocol takes a fair bit of time, and the more requests you need to make, the more you will experience that overhead.&lt;/p&gt;

&lt;p&gt;Below is a sample bulk request. It’s comprised of two parts: the action and the document. The action will include the operation to be performed such as: index, delete, create, update. It also contains additional metadata such as index or document id. The second part is the document you wish to ingest.&lt;/p&gt;

&lt;div class=&quot;language-jsx highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;POST&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;_bulk&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_index&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;test-index&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_id&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23492543&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;here&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_index&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;test-index&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_id&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23492543&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;updated&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;here&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_index&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;test-index&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_id&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23492543&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_bulk&lt;/code&gt; API there are a few things to know. First things first, you should experiment with multiple sizes for your requests. You can try with 100 items per request and move up from there in increments until the performance no longer improves. Different data has different performance characteristics so it’s important to tune your request size.&lt;/p&gt;

&lt;p&gt;Second, after you’ve found the right size for your requests its time to go multi-threaded. Many of our client libraries have asynchronous counterparts. This is important as otherwise you are limited to the amount you can ingest on a single thread on both OpenSearch’s side and client side as well.&lt;/p&gt;

&lt;p&gt;The third thing you need to know is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_bulk&lt;/code&gt; endpoint succeeds even in the event of failures. Because, you are performing several actions some may succeed and some may fail. When there are failures &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_bulk&lt;/code&gt; will send back the documents that failed. Even though the http response code may show as a 200 there may still be failures. It’s good practice to have idempotent ingestion (or it can be re-done and produce the same result). Additionally, it’s important to check the http response code for 429’s (too many requests). When you are ingesting data rapidly this can happen so check for that status code and use exponential back off to avoid overwhelming the cluster.&lt;/p&gt;

&lt;h1 id=&quot;host-settings&quot;&gt;Host Settings&lt;/h1&gt;

&lt;h3 id=&quot;jvm-optimization&quot;&gt;JVM Optimization&lt;/h3&gt;

&lt;p&gt;There are a few optimizations to look into when you are setting up your cluster for OpenSearch. For those who may not know, OpenSearch is a project written in Java and it relies on the JVM (Java Virtual Machine) to run. When the JVM starts up it uses a set amount of memory based on the default configuration (1Gb out of the box). This means that even if you attempt to run OpenSearch on a machine that has 32Gb it will not be able to take full advantage of those resources unless you adjust the settings.&lt;/p&gt;

&lt;p&gt;For ingest heavy workloads we recommend setting the JVM size to half of your available memory. There are two ways this can be set:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;JVM config file (tarball install): &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/config/jvm.options&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# /config/jvm.options&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Min JVM memory size&lt;/span&gt;
- Xms16g

&lt;span class=&quot;c&quot;&gt;# Max JVM memory size&lt;/span&gt;
- Xmx16g
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;Environment variable (container based installs): &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OPENSEARCH_JAVA_OPTS&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;OPENSEARCH_JAVA_OPTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-Xms16g -Xmx16g“
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;translog-flush&quot;&gt;Translog Flush&lt;/h3&gt;

&lt;p&gt;A translog flush is when OpenSearch commits the items that have been ingested but not persisted to on-disk Lucene segments. By default this happens every time the translog reaches 512MB. Increasing the translog flush size reduces the frequency of these operations which are resource intensive. Additionally they create larger Lucene segments which merge less frequently saving even more resources for ingestion. To see your current translog statistics you can use the &lt;a href=&quot;https://opensearch.org/docs/latest/api-reference/nodes-apis/nodes-stats/&quot;&gt;node stats&lt;/a&gt; endpoint like so: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curl -XPOST &quot;&amp;lt;cluster url&amp;gt;/&amp;lt;index name&amp;gt;/_stats/flush?pretty&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We recommend setting the translog flush size to around 25% of your available Java heap defined earlier. On a node with 16GB of memory we would set our heap to 50% of the available memory which would be 8GB for the heap. Then we would take 25% of the heap to set aside for the translog flushes or 2GB.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;index-name/_settings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;index&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;translog.flush_threshold_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2048MB&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Small note, the translog is not in memory. We set the translog threshold  to 25% of our memory because when the translog flushes it will transfer the data from disk into memory to be compiled into a Lucene segment to be persisted to disk.&lt;/p&gt;

&lt;h1 id=&quot;sharding-and-replication&quot;&gt;Sharding and Replication&lt;/h1&gt;

&lt;p&gt;We talked a little bit about threading earlier and now we will dive a little bit more into mechanisms for making things happen in parallel. OpenSearch uses shards to partition indexes. Which shard an index goes to is based off a hash of it’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_id&lt;/code&gt;. There are primary shards and replica shards. Primary shards are responsible for ingestion and sending the data to the replicas.&lt;/p&gt;

&lt;h3 id=&quot;primary-shard-distribution&quot;&gt;Primary Shard Distribution&lt;/h3&gt;

&lt;p&gt;We want to ensure we prioritize balancing our shards across nodes so the workload is evenly distributed. Without setting this we could end up with a situation like the one below where all our primary shards for end up on the same node. That would cause our Node 1 here to be completely saturated while the other nodes may be underutilized.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-20-unlocking-the-secrets-to-ingestion/PrimaryShardDistribution.svg&quot; alt=&quot;Primary Shard Distribution&quot; class=&quot;img-fluid&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;PUT&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;_cluster/settings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;persistent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cluster.routing.allocation.balanace.prefer_primary&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Enabling primary shard distribution ensures primary shards are given the first preference when it comes to spreading them across nodes. When this is enabled replicas may be more likely to end up on the same node however this is okay in an ingestion heavy workload.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-20-unlocking-the-secrets-to-ingestion/PrimaryShardDistributionEnabled.svg&quot; alt=&quot;Primary Shard Distribution Enabled&quot; class=&quot;img-fluid&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;segment-replication&quot;&gt;Segment Replication&lt;/h3&gt;

&lt;p&gt;One setting that dramatically impact ingestion is segment replication. Out of the box OpenSearch uses document replication. In this strategy data is first sent to the primary shard which then ingests it back into a Lucene segment. After it’s done that it will send the original unprocessed document to any replicas. They will then ingest the document back to their own Lucene segments. This works really well if you need more immediate consistency however it is duplicating the work done on the CPU for each replica.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-20-unlocking-the-secrets-to-ingestion/DocumentReplication.svg&quot; alt=&quot;Document Replication&quot; class=&quot;img-fluid&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With segment replication enabled the first two steps remain the same. Once it gets to the third step is where the magic happens. Instead of sending the original unprocessed document into the replicas we send the processed Lucene segment over the network. Now the nodes that the replicas live on will be free to use more of their compute for ingestion. This comes with a few caveats though. The first is you will be using more bandwidth between nodes as the segments are much larger than the initial documents. It’s important to consider your network topology here. Second, this model is an eventually consistent model as the document updates are not available as readily on replicas. The final, is you will want to increase the refresh interval of the nodes so you do not overwhelm the replica nodes with network traffic. We’ll discuss that more in the next section.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/media/blog-images/2023-11-20-unlocking-the-secrets-to-ingestion/SegmentReplication.svg&quot; alt=&quot;Segment Replication&quot; class=&quot;img-fluid&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This setting needs to be applied before the index is created. If you have an index you would like to convert to segment replication you can create a new index and use one of OpenSearch’s reindex api’s to get the data in. Here is how you could configure segment replication for an index.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;index-name/_settings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;index&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;replication.type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;SEGMENT&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;refresh-interval&quot;&gt;Refresh Interval&lt;/h3&gt;

&lt;p&gt;This setting represents amount of time between when a document is received and when it is sent to replicas. This can either be set at a cluster level or on a per-index level. By default OpenSearch uses a 1 second refresh interval but that is probably faster than needed for most people with the observability use case. Below is how you could set that in the index settings. You can change this refresh interval even after the index has already been created.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;index-name/_settings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;index&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;refresh_interval&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;30s&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we mentioned in the previous section we increase this delay so that we are sending segments less frequently. They are much larger than the original documents so sending them more quickly could end up saturating the connection of a node.&lt;/p&gt;

&lt;h1 id=&quot;compression-settings&quot;&gt;Compression Settings&lt;/h1&gt;

&lt;p&gt;When you are storing large amounts of data it can be handy to use compression to minimize on disk size. We have a few different compression algorithms that you can use but the one we will talk about is Zstandard compression (ZSTD). This compression works well because it is tunable, meaning it lets you prioritize either speed or level of compression. Below is the setting we can use to enable ZSTD compression. Note, the compression level is a default of 3 with 6 being the most compressed and 1 being the least.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;index-name/_settings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;index&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;codec&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;zstd_no_dict&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;codec.compression_level&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;end-to-end&quot;&gt;End to end&lt;/h1&gt;

&lt;p&gt;Here are the overall settings that I used (excluding the host settings, as these aren’t applied through the cluster APIs). On the client side we are using the asynchronous Python client to ensure we can saturate our network connection.&lt;/p&gt;

&lt;p&gt;For cluster settings, we balance the primary shards and enable segment replication back pressure. The back pressure setting prevents us from overwhelming the nodes if they fall behind the primary replica.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;persistent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cluster.routing.allocation.balance.prefer_primary&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;segrep.pressure.enabled&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then for our index, we increase the number of shards (and have only one replica for them). We turn on ZSTD compression to decrease the size on disk. Finally, we turn on segment replication and increase the refresh interval.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;index-name/_settings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;index&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;number_of_shards&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;number_of_replicas&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;codec&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;zstd_no_dict&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;replication.type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;SEGMENT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;refresh_interval&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;30s&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With all of these settings we were able to see a 65% improvement in ingestion speed and a 19% reduction of the size of our data on disk. These are some pretty serious gains for just doing some straightforward settings updates. While your experience may be different, you now can tune your ingestion to fit the needs of your dataset! Check out &lt;a href=&quot;https://opensearch.org/docs/latest/tuning-your-cluster/performance/&quot;&gt;Tuning your cluster for indexing speed&lt;/a&gt; for a deeper dive on some of these strategies.&lt;/p&gt;

&lt;p&gt;If enjoyed this post and want to see it with some more detail, you can check out my presentation from OpenSearchCon 2023 that this blog was drawn from:&lt;/p&gt;

&lt;!--
Copyright (c) 2020 Nathan Lam
https://github.com/nathancy/jekyll-embed-video
--&gt;

&lt;div class=&quot;embed-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/xXEXnNIcvTg&quot; width=&quot; 640&quot; height=&quot;385&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; allow=&quot;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot;&gt;
    &lt;/iframe&gt;
  &lt;/div&gt;</content><author><name>dtaivpp</name></author><category term="ingestion" /><category term="clients" /><summary type="html">While most of the optimization secrets I’m sharing with you aren’t intentionally hidden, they sure felt hidden to me when I started with OpenSearch.</summary></entry></feed>